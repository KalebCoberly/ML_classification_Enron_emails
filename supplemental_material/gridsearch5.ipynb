{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dos2unix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-442c50319c17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m### My imports\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tools/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdos2unix\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcrlf_to_lf\u001b[0m \u001b[1;31m# Borrowed and modified from multiple sources.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtrain_test\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrun_skl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_base_perfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_em_all\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfeature_engineering\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_all_ratios\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquant_flag_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_flag_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag_signs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_k_means_n\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dos2unix'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "# from time import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import math\n",
    "from scipy import stats\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "\n",
    "from sklearn.feature_selection import SelectPercentile, SelectFromModel, f_classif, mutual_info_classif, chi2,\\\n",
    "                                        SelectFpr, SelectFdr, RFECV\n",
    "from sklearn.decomposition import FastICA, IncrementalPCA, KernelPCA, PCA, TruncatedSVD\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "### My imports\n",
    "sys.path.append('tools/')\n",
    "from dos2unix import crlf_to_lf # Borrowed and modified from multiple sources.\n",
    "from train_test import run_skl, get_base_perfs, search_em_all\n",
    "from feature_engineering import set_all_ratios, quant_flag_all, out_flag_all, flag_signs, add_k_means_n\n",
    "\n",
    "### Udacity imports\n",
    "# from feature_format import featureFormat, targetFeatureSplit # (may be modified)\n",
    "# from tester import dump_classifier_and_data\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/final_project_dataset.pkl saved as data/final_project_dataset_unix.pkl in 6705 bytes.\n"
     ]
    }
   ],
   "source": [
    "##########################################################################################\n",
    "### Load the dictionary containing the dataset, and clean it up.\n",
    "### Make the dict a dataframe because they're easier to work with.\n",
    "data_df = None #pd.DataFrame()\n",
    "fp = crlf_to_lf(f_in_path='data/final_project_dataset.pkl')\n",
    "with open(fp, 'rb') as data_file:\n",
    "    data_df = pd.DataFrame(pickle.load(data_file)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "### Task 1: Clean up and select what features and subsets *not* to use.\n",
    "### (Further feature selection will happen after feature engineering.)\n",
    "    \n",
    "### Drop email_address since it's a signature.\n",
    "data_df.drop(columns='email_address', inplace=True)\n",
    "### Drop the TOTAL row.\n",
    "data_df.drop(labels=['TOTAL', 'THE TRAVEL AGENCY IN THE PARK'], inplace=True)\n",
    "\n",
    "### Handle missing values here.\n",
    "### Replacing 'NaN' with None had a weird result in which values from some\n",
    "### rows were copied into the missing values of neighboring rows. No idea why.\n",
    "### Using np.nan did not have that result as far as I can tell.\n",
    "### But it is a float missing value and thus casts the column as float,\n",
    "### or as object when other values are not floats.\n",
    "data_df.replace(to_replace='NaN', value=np.nan, inplace=True)\n",
    "\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\"\n",
    "###    (if using featureFormat(), which I don't).\n",
    "\n",
    "### All units are in USD.\n",
    "fin_features = ['salary', 'bonus', 'long_term_incentive', 'deferred_income', 'deferral_payments',\n",
    "                'loan_advances', 'other', 'expenses', 'director_fees', 'total_payments',\n",
    "                'exercised_stock_options', 'restricted_stock', 'restricted_stock_deferred', 'total_stock_value']\n",
    "pay_features = fin_features[:10]\n",
    "stock_features = fin_features[10:]\n",
    "    \n",
    "### Units are number of emails messages;\n",
    "email_features = ['to_messages', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi',\n",
    "                  'shared_receipt_with_poi']\n",
    "\n",
    "### Boolean, represented as integer.\n",
    "POI_label = ['poi']\n",
    "\n",
    "### The first feature must be \"poi\" if using featureFormat().\n",
    "features_list = POI_label + fin_features + email_features\n",
    "\n",
    "### Imputation recasts as float, but as object if left as bool, so set it to int for now.\n",
    "data_df['poi'] = data_df['poi'].astype(dtype=int)\n",
    "\n",
    "### Belfer's financial data is shifted one column to the right.\n",
    "### Shift it one to the left, financial data only.\n",
    "### Make total_stock_value np.nan for consistency until imputation, but could be 0.\n",
    "### May remove this row for so many NaNs, but fix it now anyway.\n",
    "data_df.loc[data_df.index == 'BELFER ROBERT', fin_features] \\\n",
    "    = data_df.loc[data_df.index == 'BELFER ROBERT', fin_features].shift(periods=-1, axis='columns',\n",
    "                                                                        fill_value=np.nan)\n",
    "\n",
    "### Bhatnagar's financial data is shifted one to the left.\n",
    "### Shift it one to the right, financial data only.\n",
    "### Make salary np.nan.\n",
    "data_df.loc[data_df.index == 'BHATNAGAR SANJAY', fin_features] \\\n",
    "    = data_df.loc[data_df.index == 'BHATNAGAR SANJAY', fin_features].shift(periods=1, axis='columns',\n",
    "                                                                           fill_value=np.nan)\n",
    "\n",
    "### Set totals to sum of values where any values are not NaN.\n",
    "### i.e. don't make 0 totals NaN, even though some NaN values may be included.\n",
    "### Makes these rows consistent with other rows that include NaNs and numbers yet have a nonNaN total.\n",
    "data_df.loc[~(data_df[pay_features].isna().all(axis='columns')), 'total_payments'] \\\n",
    "    = data_df[pay_features[:-1]].sum(axis='columns')\n",
    "data_df.loc[~(data_df[stock_features].isna().all(axis='columns')), 'total_stock_value'] \\\n",
    "    = data_df[stock_features[:-1]].sum(axis='columns')\n",
    "\n",
    "### Add one to Glisan's to_message to at least equal shared_receipt_with_poi.\n",
    "data_df.loc['GLISAN JR BEN F', 'to_messages'] = 874\n",
    "\n",
    "### Drop features that are too sparse.\n",
    "drop_feats_lst = ['loan_advances']\n",
    "data_df.drop(columns=drop_feats_lst, inplace=True)\n",
    "fin_features = [feat for feat in fin_features if feat not in drop_feats_lst]\n",
    "pay_features = [feat for feat in pay_features if feat not in drop_feats_lst]\n",
    "stock_features = [feat for feat in stock_features if feat not in drop_feats_lst]\n",
    "email_features = [feat for feat in email_features if feat not in drop_feats_lst]\n",
    "features_list = [feat for feat in features_list if feat not in drop_feats_lst]\n",
    "\n",
    "### Removed 'email' as signature upon loading.\n",
    "\n",
    "### Drop persons who have NaN payment totals or NaN stock totals or NaN to_messages or NaN from_messages,\n",
    "### and are missing 70% of their values.\n",
    "### (Already made sure that all totals are not NaN if they have subvalues.)\n",
    "nan_limit = 0.7 * len(data_df.columns)\n",
    "sparse_records_idx_arr = \\\n",
    "    data_df.loc[data_df['total_payments'].isna() \\\n",
    "                | data_df['total_stock_value'].isna() \\\n",
    "                | data_df['to_messages'].isna() \\\n",
    "                | data_df['from_messages'].isna()]\\\n",
    "           .loc[data_df.isna().sum(axis='columns') > nan_limit]\\\n",
    "           .index.values\n",
    "data_df.drop(labels=sparse_records_idx_arr, inplace=True)\n",
    "\n",
    "### This leaves 123 records over 19 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline model performance metrics:\n",
      "\n",
      "DecisionTreeClassifier()\n",
      "Training time: 0.002 s\n",
      "Prediction time: 0.001 s\n",
      "Confusion matrix:\n",
      " [[18  4]\n",
      " [ 3  1]]\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.85714286, 0.2       ]), array([0.81818182, 0.25      ]), array([0.8372093 , 0.22222222]), array([22,  4], dtype=int64))\n",
      "RandomForestClassifier()\n",
      "Training time: 0.094 s\n",
      "Prediction time: 0.007 s\n",
      "Confusion matrix:\n",
      " [[22  0]\n",
      " [ 4  0]]\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.84615385, 0.        ]), array([1., 0.]), array([0.91666667, 0.        ]), array([22,  4], dtype=int64))\n",
      "AdaBoostClassifier()\n",
      "Training time: 0.054 s\n",
      "Prediction time: 0.007 s\n",
      "Confusion matrix:\n",
      " [[20  2]\n",
      " [ 3  1]]\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.86956522, 0.33333333]), array([0.90909091, 0.25      ]), array([0.88888889, 0.28571429]), array([22,  4], dtype=int64))\n",
      "KNeighborsClassifier()\n",
      "Training time: 0.002 s\n",
      "Prediction time: 0.003 s\n",
      "Confusion matrix:\n",
      " [[22  0]\n",
      " [ 4  0]]\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.84615385, 0.        ]), array([1., 0.]), array([0.91666667, 0.        ]), array([22,  4], dtype=int64))\n",
      "GaussianNB()\n",
      "Training time: 0.002 s\n",
      "Prediction time: 0.001 s\n",
      "Confusion matrix:\n",
      " [[16  6]\n",
      " [ 1  3]]\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.94117647, 0.33333333]), array([0.72727273, 0.75      ]), array([0.82051282, 0.46153846]), array([22,  4], dtype=int64))\n",
      "SVC()\n",
      "Training time: 0.001 s\n",
      "Prediction time: 0.001 s\n",
      "Confusion matrix:\n",
      " [[22  0]\n",
      " [ 4  0]]\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.84615385, 0.        ]), array([1., 0.]), array([0.91666667, 0.        ]), array([22,  4], dtype=int64))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaleb\\anaconda3\\envs\\py3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kaleb\\anaconda3\\envs\\py3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kaleb\\anaconda3\\envs\\py3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "### Make a quick baseline model for comparison.\n",
    "\n",
    "### Alphabetize index before split for Udacity compatibility because that's what they'll do.\n",
    "### I knew this, but missed it until the end. I'd decided from the start not to use their deprecated\n",
    "### scripts based on dictionaries, specifically feature_format, and wrote my own using pandas. \n",
    "### The rest of my cleaning and engineering are based on a different split.\n",
    "### My mistake. Lesson learned: pay closer attention to what the legacy code does, especially \n",
    "### expected input/output structures.\n",
    "data_df.sort_index(inplace=True)\n",
    "\n",
    "### Impute with 0.\n",
    "imp_0 = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0, copy=False)\n",
    "imp_0 = imp_0.fit(X=data_df)\n",
    "data_imp0_df = pd.DataFrame(data=imp_0.transform(X=data_df), columns=data_df.columns, index=data_df.index)\n",
    "\n",
    "### Split now for baseline model, but also before further processing, outlier removal, scaling, engineering,\n",
    "### or else test set info leaks into training set.\n",
    "### Even imputation could if using multivariate imputation or median.\n",
    "### Decision on how to treat the data should not be influenced by test set either.\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_imp0_df[features_list[1:]], data_imp0_df[['poi']],\n",
    "                                                    test_size=.3, random_state=42)\n",
    "### Some algorithms want 1D y data.\n",
    "y_train_1d = np.ravel(y_train.astype(bool))\n",
    "y_test_1d = np.ravel(y_test.astype(bool))\n",
    "\n",
    "### Split train set again for a baseline model that won't touch the final test set.\n",
    "X_train_base, X_test_base, y_train_base, y_test_base \\\n",
    "    = train_test_split(X_train, y_train, test_size=.3, random_state=42)\n",
    "y_train_1d_base = np.ravel(y_train_base.astype(bool))\n",
    "y_test_1d_base = np.ravel(y_test_base.astype(bool))\n",
    "\n",
    "### For metrics.\n",
    "ordered_cols_lst = ['nonPOI_prec', 'POI_prec', 'nonPOI_rec', 'POI_rec', 'nonPOI_f', 'POI_f', 'nonPOI_sup',\n",
    "                    'POI_sup', 't_neg', 'f_neg', 'f_pos', 't_pos', 'train_t', 'predict_t', 'model']\n",
    "base_perf_df = pd.DataFrame(columns=ordered_cols_lst)\n",
    "\n",
    "clf_dict = {'dt_clf': DecisionTreeClassifier, 'rf_clf': RandomForestClassifier, 'ab_clf': AdaBoostClassifier,\n",
    "            'kn_clf': KNeighborsClassifier, 'gnb_clf': GaussianNB, 'svc_clf': svm.SVC}\n",
    "\n",
    "print('\\nBaseline model performance metrics:\\n')\n",
    "for key, method in clf_dict.items():\n",
    "    _, _, _, _, perf_sr = run_skl(method=method, X_train=X_train_base,\n",
    "                                  y_train=y_train_1d_base,\n",
    "                                  X_test=X_test_base,\n",
    "                                  y_test=y_test_1d_base,\n",
    "                                  perf_series=key)\n",
    "    base_perf_df = base_perf_df.append(perf_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "### Task 2: Remove/handle outliers\n",
    "\n",
    "### Dropped ['TOTAL', 'THE TRAVEL AGENCY IN THE PARK'] row upon loading.\n",
    "\n",
    "### Drop features that are too sparse.\n",
    "### Drop 'other' because it's ill-defined and seems overly represented within important features. The nebulous nature of it seems like a good fit for fraud, but high gross 'other' amounts are more correlated with nonPOIs than POIs if anything.\n",
    "drop_feats_lst = ['director_fees', 'restricted_stock_deferred', 'other']\n",
    "\n",
    "X_train.drop(columns=drop_feats_lst, inplace=True)\n",
    "X_test.drop(columns=drop_feats_lst, inplace=True)\n",
    "data_df.drop(columns=drop_feats_lst, inplace=True)\n",
    "\n",
    "fin_features = [feat for feat in fin_features if feat not in drop_feats_lst]\n",
    "pay_features = [feat for feat in pay_features if feat not in drop_feats_lst]\n",
    "stock_features = [feat for feat in stock_features if feat not in drop_feats_lst]\n",
    "email_features = [feat for feat in email_features if feat not in drop_feats_lst]\n",
    "features_list = [feat for feat in features_list if feat not in drop_feats_lst]\n",
    "del drop_feats_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Don't drop records now because it will mess up the split for Udacity.\n",
    "### Could drop earlier and resplit, but I've already done a lot of EDA behind the scenes.\n",
    "### NaN his financials.\n",
    "X_train.loc[['POWERS WILLIAM'], pay_features] = np.nan\n",
    "data_df.loc[['POWERS WILLIAM'], pay_features] = np.nan\n",
    "\n",
    "### Bivariate linear regression of the ratios between to/from/shared with POIs and\n",
    "### total to and from messages revealed that top coding to_messages and from_messages\n",
    "### may slightly aid nonPOI precision.\n",
    "### Only top coding the training set in order to bias the model,\n",
    "### since I am less concerned with accuracy than I am with POI recall,\n",
    "### and by extension, nonPOI precision.\n",
    "X_train['to_messages'] = X_train['to_messages'].apply(lambda x: x if x < 12000 or np.isnan(x) else 12000)\n",
    "X_train['from_messages'] = X_train['from_messages'].apply(lambda x: x if x < 8000 or np.isnan(x) else 8000)\n",
    "data_df.loc[X_train.index]['to_messages'] \\\n",
    "    = data_df.loc[X_train.index]['to_messages'].apply(lambda x: x if x < 12000 or np.isnan(x) else 12000)\n",
    "data_df.loc[X_train.index]['from_messages'] \\\n",
    "    = data_df.loc[X_train.index]['from_messages'].apply(lambda x: x if x < 8000 or np.isnan(x) else 8000)\n",
    "\n",
    "### Not sure whether top coding these will really help or hinder, if anything at all.\n",
    "### But, it appears to potentially aid POI recall in some cases\n",
    "### when comparing payments to totals, and it's more in line with best practices.\n",
    "### Only really affects Frevert.\n",
    "top = X_train['total_payments'].dropna().sort_values()[-2]\n",
    "X_train['total_payments'] = X_train['total_payments'].apply(lambda x : x if x < top or np.isnan(x) else top)\n",
    "data_df.loc[X_train.index]['total_payments'] \\\n",
    "    = data_df.loc[X_train.index]['total_payments'].apply(lambda x : x if x < top or np.isnan(x) else top)\n",
    "\n",
    "top = X_train['long_term_incentive'].dropna().sort_values()[-2]\n",
    "X_train['long_term_incentive'] = \\\n",
    "    X_train['long_term_incentive'].apply(lambda x : x if x < top or np.isnan(x) else top)\n",
    "data_df.loc[X_train.index]['long_term_incentive'] \\\n",
    "    = data_df.loc[X_train.index]['long_term_incentive'].apply(lambda x : x if x < top or np.isnan(x) else top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Same story as Powers, NaN all of Belfer instead of simply dropping.\n",
    "X_train.loc['BELFER ROBERT'] = np.nan\n",
    "# belfers_poi = data_df.loc['BELFER ROBERT']['poi']\n",
    "data_df.loc['BELFER ROBERT', features_list[1:]]= np.nan\n",
    "# data_df.loc['BELFER ROBERT']['poi'] = belfers_poi\n",
    "\n",
    "### After look at distributions of ratios of features, more top/bottom coding. ###\n",
    "\n",
    "### Nan Bannantine's salary, and bottom code salary.\n",
    "X_train.loc['BANNANTINE JAMES M', 'salary'] = np.nan\n",
    "data_df.loc['BANNANTINE JAMES M', 'salary'] = np.nan\n",
    "bottom = X_train['salary'].dropna().sort_values(ascending=False)[-2]\n",
    "X_train['salary'] = X_train['salary'].apply(lambda x : x if x > bottom or np.isnan(x) else bottom)\n",
    "data_df.loc[X_train.index]['salary'] \\\n",
    "    = data_df.loc[X_train.index]['salary'].apply(lambda x : x if x > bottom or np.isnan(x) else bottom)\n",
    "\n",
    "### These two only have one, very low payment value.\n",
    "# X_train.loc[['HAYES ROBERT E', 'HAUG DAVID L'], pay_features] = np.nan\n",
    "# data_df.loc[['HAYES ROBERT E', 'HAUG DAVID L'], pay_features] = np.nan\n",
    "X_train.loc[['HAYES ROBERT E'], pay_features] = np.nan\n",
    "data_df.loc[['HAYES ROBERT E'], pay_features] = np.nan\n",
    "\n",
    "### Top code deferred_income.\n",
    "top = X_train['deferred_income'].dropna().sort_values(ascending=True)[-3]\n",
    "X_train['deferred_income'] = X_train['deferred_income'].apply(lambda x : x if x < top or np.isnan(x) else top)\n",
    "data_df.loc[X_train.index]['deferred_income'] = \\\n",
    "    data_df.loc[X_train.index]['deferred_income'].apply(lambda x : x if x < top or np.isnan(x) else top)\n",
    "del top\n",
    "del bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "### Task 3: Create new feature(s)\n",
    "\n",
    "\n",
    "### Start with all ratios, within respective subspaces (fin:fin, e:e).\n",
    "### Add financial ratios within subspaces to data sets.\n",
    "pay_feats_divby_df = set_all_ratios(df=X_train, denoms=pay_features, numers=pay_features)\n",
    "stock_feats_divby_df = set_all_ratios(df=X_train, denoms=stock_features, numers=stock_features)\n",
    "\n",
    "### Only plausible email ratios (all reciprocals still, to get the 0s to infs):\n",
    "to_lst = ['to_messages', 'from_poi_to_this_person', 'shared_receipt_with_poi']\n",
    "from_lst = ['from_messages', 'from_this_person_to_poi']\n",
    "email_to_divby_df = set_all_ratios(df=X_train, denoms=to_lst, numers=to_lst)\n",
    "email_from_divby_df = set_all_ratios(df=X_train, denoms=from_lst, numers=from_lst)\n",
    "\n",
    "X_train = pd.concat(objs=[X_train, pay_feats_divby_df, stock_feats_divby_df, email_to_divby_df,\n",
    "                          email_from_divby_df], axis=1)\n",
    "\n",
    "### Do for test set.\n",
    "pay_feats_divby_df = set_all_ratios(df=X_test, denoms=pay_features, numers=pay_features)\n",
    "stock_feats_divby_df = set_all_ratios(df=X_test, denoms=stock_features, numers=stock_features)\n",
    "email_to_divby_df = set_all_ratios(df=X_test, denoms=to_lst, numers=to_lst)\n",
    "email_from_divby_df = set_all_ratios(df=X_test, denoms=from_lst, numers=from_lst)\n",
    "X_test = pd.concat(objs=[X_test, pay_feats_divby_df, stock_feats_divby_df, email_to_divby_df,\n",
    "                         email_from_divby_df], axis=1)\n",
    "\n",
    "### Do for full set.\n",
    "pay_feats_divby_df = set_all_ratios(df=data_df, denoms=pay_features, numers=pay_features)\n",
    "stock_feats_divby_df = set_all_ratios(df=data_df, denoms=stock_features, numers=stock_features)\n",
    "email_to_divby_df = set_all_ratios(df=data_df, denoms=to_lst, numers=to_lst)\n",
    "email_from_divby_df = set_all_ratios(df=data_df, denoms=from_lst, numers=from_lst)\n",
    "data_df = pd.concat(objs=[data_df, pay_feats_divby_df, stock_feats_divby_df, email_to_divby_df,\n",
    "                          email_from_divby_df], axis=1)\n",
    "del to_lst\n",
    "del from_lst\n",
    "\n",
    "### Set all np.inf to np.nan.\n",
    "X_train = X_train.apply(func=(lambda col: col.apply(func=(lambda x: np.nan if abs(x) == abs(np.inf) else x))))\n",
    "X_test = X_test.apply(func=(lambda col: col.apply(func=(lambda x: np.nan if abs(x) == abs(np.inf) else x))))\n",
    "data_df = data_df.apply(func=(lambda col: col.apply(func=(lambda x: np.nan if abs(x) == abs(np.inf) else x))))\n",
    "\n",
    "### Remove all features containing less than 30% training observations.\n",
    "drop_lst = list(X_train.count().loc[X_train.count() < .3 * len(X_train.index)].index)\n",
    "X_train.drop(columns=drop_lst, inplace=True)\n",
    "X_test.drop(columns=drop_lst, inplace=True)\n",
    "data_df.drop(columns=drop_lst, inplace=True)\n",
    "\n",
    "pay_feats_divby_lst = [feat for feat in list(pay_feats_divby_df.columns) if not feat in drop_lst]\n",
    "stock_feats_divby_lst = [feat for feat in list(stock_feats_divby_df.columns) if not feat in drop_lst]\n",
    "email_feats_divby_lst = [feat for feat in list(email_to_divby_df.columns) if not feat in drop_lst] \\\n",
    "                        + [feat for feat in list(email_from_divby_df.columns) if not feat in drop_lst]\n",
    "fin_features = [feat for feat in fin_features if feat not in drop_lst] + pay_feats_divby_lst \\\n",
    "    + stock_feats_divby_lst\n",
    "pay_features = [feat for feat in pay_features if feat not in drop_lst]\n",
    "stock_features = [feat for feat in stock_features if feat not in drop_lst]\n",
    "email_features = [feat for feat in email_features if feat not in drop_lst] + email_feats_divby_lst\n",
    "features_list = [feat for feat in features_list if feat not in drop_lst] + pay_feats_divby_lst \\\n",
    "    + stock_feats_divby_lst + email_feats_divby_lst\n",
    "del drop_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create features that flag mambership in various quantiles, outliership, and x > 0.\n",
    "### Use multiple quantiles: quartiles, quintiles, and deciles.\n",
    "### Retain np.nans.\n",
    "\n",
    "to_flag_lst = fin_features + email_features\n",
    "\n",
    "### Could write a function, but I'll just paste and edit.\n",
    "### Flag train set.\n",
    "fin_quant_flags_df = quant_flag_all(df=X_train[fin_features], quant_df=X_train[fin_features])\n",
    "email_quant_flags_df = quant_flag_all(df=X_train[email_features], quant_df=X_train[email_features])\n",
    "fin_out_flags_df = out_flag_all(df=X_train[fin_features], quant_df=X_train[fin_features])\n",
    "email_out_flags_df = out_flag_all(df=X_train[email_features], quant_df=X_train[email_features])\n",
    "sign_flags_df = flag_signs(df=X_train[to_flag_lst])\n",
    "X_train = pd.concat(objs=[X_train, fin_quant_flags_df, email_quant_flags_df, fin_out_flags_df,\n",
    "                          email_out_flags_df, sign_flags_df], axis=1)\n",
    "\n",
    "### Flag test set.\n",
    "fin_quant_flags_df = quant_flag_all(df=X_test[fin_features], quant_df=X_train[fin_features])\n",
    "email_quant_flags_df = quant_flag_all(df=X_test[email_features], quant_df=X_train[email_features])\n",
    "fin_out_flags_df = out_flag_all(df=X_test[fin_features], quant_df=X_train[fin_features])\n",
    "email_out_flags_df = out_flag_all(df=X_test[email_features], quant_df=X_train[email_features])\n",
    "sign_flags_df = flag_signs(df=X_test[to_flag_lst])\n",
    "X_test = pd.concat(objs=[X_test, fin_quant_flags_df, email_quant_flags_df, fin_out_flags_df,\n",
    "                          email_out_flags_df, sign_flags_df], axis=1)\n",
    "\n",
    "### Flag whole set.\n",
    "fin_quant_flags_df = quant_flag_all(df=data_df[fin_features], quant_df=X_train[fin_features])\n",
    "email_quant_flags_df = quant_flag_all(df=data_df[email_features], quant_df=X_train[email_features])\n",
    "fin_out_flags_df = out_flag_all(df=data_df[fin_features], quant_df=X_train[fin_features])\n",
    "email_out_flags_df = out_flag_all(df=data_df[email_features], quant_df=X_train[email_features])\n",
    "sign_flags_df = flag_signs(df=data_df[to_flag_lst])\n",
    "data_df = pd.concat(objs=[data_df, fin_quant_flags_df, email_quant_flags_df, fin_out_flags_df,\n",
    "                          email_out_flags_df, sign_flags_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create and update feature lists.\n",
    "fin_quant_flags_lst = list(fin_quant_flags_df.columns)\n",
    "email_quant_flags_lst = list(email_quant_flags_df.columns)\n",
    "quant_flags_lst = fin_quant_flags_lst + email_quant_flags_lst\n",
    "\n",
    "fin_out_flags_lst = list(fin_out_flags_df.columns)\n",
    "email_out_flags_lst = list(email_out_flags_df.columns)\n",
    "out_flags_lst = fin_out_flags_lst + email_out_flags_lst\n",
    "\n",
    "fin_features += fin_quant_flags_lst + fin_out_flags_lst\n",
    "email_features += email_quant_flags_lst + email_out_flags_lst\n",
    "\n",
    "sign_flags_lst = list(sign_flags_df.columns)\n",
    "\n",
    "features_list = features_list + quant_flags_lst + out_flags_lst + sign_flags_lst\n",
    "\n",
    "del to_flag_lst\n",
    "del fin_quant_flags_df\n",
    "del email_quant_flags_df\n",
    "del fin_out_flags_df\n",
    "del email_out_flags_df\n",
    "del sign_flags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scale features\n",
    "### Just do min-max on floats, not bools (some are objects for now because np.nan)\n",
    "\n",
    "float_feats_lst = fin_features + email_features\n",
    "bool_feats_lst =  sign_flags_lst\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_floats = pd.DataFrame(data=scaler.fit_transform(X=X_train[float_feats_lst]),\n",
    "                            columns=float_feats_lst, index=X_train.index)\n",
    "X_train_scaled = pd.concat(objs=[train_floats, X_train[bool_feats_lst]], axis=1)\n",
    "\n",
    "test_floats = pd.DataFrame(data=scaler.transform(X=X_test[float_feats_lst]),\n",
    "                           columns=float_feats_lst,index=X_test.index)\n",
    "X_test_scaled = pd.concat(objs=[test_floats, X_test[bool_feats_lst]], axis=1)\n",
    "\n",
    "all_floats = pd.DataFrame(data=scaler.transform(X=data_df[float_feats_lst]),\n",
    "                          columns=float_feats_lst, index=data_df.index)\n",
    "data_df_scaled = pd.concat(objs=[data_df['poi'], all_floats, data_df[bool_feats_lst]], axis=1)\n",
    "\n",
    "del float_feats_lst\n",
    "del scaler\n",
    "del train_floats\n",
    "del test_floats\n",
    "del all_floats\n",
    "del X_train\n",
    "del X_test\n",
    "del data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Impute missing values:\n",
    "### Financial features to 0, email features to median, and bools to mode.\n",
    "### Restore bools to bool (from object because np.nan)\n",
    "\n",
    "imp0 = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "imp_med = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imp_mod = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "\n",
    "### Financial features to 0.\n",
    "fin_train_df = pd.DataFrame(data=imp0.fit_transform(X=X_train_scaled[fin_features]),\n",
    "                        columns=fin_features, index=X_train_scaled.index)\n",
    "fin_test_df = pd.DataFrame(data=imp0.transform(X=X_test_scaled[fin_features]),\n",
    "                       columns=fin_features, index=X_test_scaled.index)\n",
    "fin_all_df = pd.DataFrame(data=imp0.transform(X=data_df_scaled[fin_features]),\n",
    "                      columns=fin_features, index=data_df_scaled.index)\n",
    "\n",
    "### email features to median\n",
    "email_train_df = pd.DataFrame(data=imp_med.fit_transform(X=X_train_scaled[email_features]),\n",
    "                        columns=email_features, index=X_train_scaled.index)\n",
    "email_test_df = pd.DataFrame(data=imp_med.transform(X=X_test_scaled[email_features]),\n",
    "                       columns=email_features, index=X_test_scaled.index)\n",
    "email_all_df = pd.DataFrame(data=imp_med.transform(X=data_df_scaled[email_features]),\n",
    "                      columns=email_features, index=data_df_scaled.index)\n",
    "\n",
    "### Bools to mode.\n",
    "### Restore bools to bool (from object because np.nan)\n",
    "bool_train_df = (pd.DataFrame(data=imp_mod.fit_transform(X=X_train_scaled[bool_feats_lst]),\n",
    "                              columns=bool_feats_lst, index=X_train_scaled.index)).astype(bool)\n",
    "bool_test_df = pd.DataFrame(data=imp_mod.transform(X=X_test_scaled[bool_feats_lst]),\n",
    "                            columns=bool_feats_lst, index=X_test_scaled.index).astype(bool)\n",
    "bool_all_df = pd.DataFrame(data=imp_mod.transform(X=data_df_scaled[bool_feats_lst]),\n",
    "                           columns=bool_feats_lst, index=data_df_scaled.index).astype(bool)\n",
    "\n",
    "### Concat\n",
    "X_train_scaled_imp = pd.concat(objs=[fin_train_df, email_train_df, bool_train_df], axis=1)\n",
    "X_test_scaled_imp = pd.concat(objs=[fin_test_df, email_test_df, bool_test_df], axis=1)\n",
    "data_df_scaled_imp = pd.concat(objs=[data_df_scaled['poi'], fin_all_df, email_all_df, bool_all_df], axis=1)\n",
    "\n",
    "del fin_train_df\n",
    "del email_train_df\n",
    "del bool_train_df\n",
    "del fin_test_df\n",
    "del email_test_df\n",
    "del bool_test_df\n",
    "del fin_all_df\n",
    "del email_all_df\n",
    "del bool_all_df\n",
    "del bool_feats_lst\n",
    "del X_train_scaled\n",
    "del X_test_scaled\n",
    "del data_df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sklearn predictions as features\n",
    "\n",
    "# 1) Kmeans cluster.\n",
    "train_cluster_subspace, test_cluster_subspace \\\n",
    "    = add_k_means_n(X_train=X_train_scaled_imp, X_test=X_test_scaled_imp)\n",
    "X_train_scaled_imp_k = pd.concat(objs=[X_train_scaled_imp, train_cluster_subspace], axis=1)\n",
    "X_test_scaled_imp_k = pd.concat(objs=[X_test_scaled_imp, test_cluster_subspace], axis=1)\n",
    "\n",
    "train_cluster_subspace, test_cluster_subspace \\\n",
    "    = add_k_means_n(X_train=X_train_scaled_imp, X_test=data_df_scaled_imp[features_list[1:]])\n",
    "data_df_scaled_imp_k = pd.concat(objs=[data_df_scaled_imp, test_cluster_subspace], axis=1)\n",
    "\n",
    "k_means_feats_lst = k_means_feats_lst = list(train_cluster_subspace.columns)\n",
    "features_list += k_means_feats_lst\n",
    "\n",
    "del train_cluster_subspace\n",
    "del test_cluster_subspace\n",
    "del X_train_scaled_imp\n",
    "del X_test_scaled_imp\n",
    "del data_df_scaled_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " base_perf_engineered\n",
      "\n",
      " dt_clf\n",
      "DecisionTreeClassifier()\n",
      "Training time: 0.007 s\n",
      "Prediction time: 0.003 s\n",
      "Confusion matrix:\n",
      " [[18  4]\n",
      " [ 1  3]]\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.94736842, 0.42857143]), array([0.81818182, 0.75      ]), array([0.87804878, 0.54545455]), array([22,  4], dtype=int64))\n",
      "\n",
      " rf_clf\n",
      "RandomForestClassifier()\n",
      "Training time: 0.142 s\n",
      "Prediction time: 0.009 s\n",
      "Confusion matrix:\n",
      " [[22  0]\n",
      " [ 4  0]]\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.84615385, 0.        ]), array([1., 0.]), array([0.91666667, 0.        ]), array([22,  4], dtype=int64))\n",
      "\n",
      " ab_clf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaleb\\anaconda3\\envs\\py3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier()\n",
      "Training time: 0.116 s\n",
      "Prediction time: 0.018 s\n",
      "Confusion matrix:\n",
      " [[20  2]\n",
      " [ 3  1]]\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.86956522, 0.33333333]), array([0.90909091, 0.25      ]), array([0.88888889, 0.28571429]), array([22,  4], dtype=int64))\n",
      "\n",
      " kn_clf\n",
      "KNeighborsClassifier()\n",
      "Training time: 0.005 s\n",
      "Prediction time: 0.005 s\n",
      "Confusion matrix:\n",
      " [[21  1]\n",
      " [ 3  1]]\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.875, 0.5  ]), array([0.95454545, 0.25      ]), array([0.91304348, 0.33333333]), array([22,  4], dtype=int64))\n",
      "\n",
      " gnb_clf\n",
      "GaussianNB()\n",
      "Training time: 0.004 s\n",
      "Prediction time: 0.004 s\n",
      "Confusion matrix:\n",
      " [[18  4]\n",
      " [ 2  2]]\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.9       , 0.33333333]), array([0.81818182, 0.5       ]), array([0.85714286, 0.4       ]), array([22,  4], dtype=int64))\n",
      "\n",
      " svc_clf\n",
      "SVC()\n",
      "Training time: 0.005 s\n",
      "Prediction time: 0.003 s\n",
      "Confusion matrix:\n",
      " [[22  0]\n",
      " [ 4  0]]\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.84615385, 0.        ]), array([1., 0.]), array([0.91666667, 0.        ]), array([22,  4], dtype=int64))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaleb\\anaconda3\\envs\\py3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "### Construct baseline performance with all features before tuning/selection.\n",
    "### Split train set again for a baseline model that won't touch the final test set.\n",
    "X_train_base, X_test_base, y_train_base, y_test_base \\\n",
    "    = train_test_split(X_train_scaled_imp_k, y_train, test_size=.3, random_state=42)\n",
    "y_train_1d_base = np.ravel(y_train_base.astype(bool))\n",
    "y_test_1d_base = np.ravel(y_test_base.astype(bool))\n",
    "\n",
    "base_perf_engineered_df = pd.DataFrame(columns=ordered_cols_lst)\n",
    "\n",
    "base_perfs_dict = {'base_perf_engineered': base_perf_engineered_df}\n",
    "imp_sets_dict = {'base_perf_engineered': [X_train_base, X_test_base]}\n",
    "\n",
    "### Modifies the base_perfs_dict in place, since dict has no deep copy method.\n",
    "get_base_perfs(base_perfs_dict=base_perfs_dict, imp_sets_dict=imp_sets_dict, clf_dict=clf_dict, y_train=y_train_1d_base,\n",
    "               y_test=y_test_1d_base)\n",
    "\n",
    "base_perfs_dict['first_base'] = base_perf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0 sel_per_empty_dt_clf \n",
      "\n",
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of 105 | elapsed:    2.3s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 105 out of 105 | elapsed:    2.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........... (step 1 of 2) Processing sel_per, total=   0.1s\n",
      "[Pipeline] ............ (step 2 of 2) Processing dt_clf, total=   0.0s\n",
      "\n",
      " GridSearchCV(estimator=Pipeline(steps=[('sel_per', SelectPercentile()),\n",
      "                                       ('dt_clf', DecisionTreeClassifier())],\n",
      "                                verbose=True),\n",
      "             n_jobs=-1,\n",
      "             param_grid={'dt_clf__random_state': [42],\n",
      "                         'sel_per__percentile': [2, 4, 8, 16, 32, 64, 100],\n",
      "                         'sel_per__score_func': [<function f_classif at 0x0000029405AD6430>,\n",
      "                                                 <function chi2 at 0x0000029405AD6790>,\n",
      "                                                 functools.partial(<function mutual_info_classif at 0x0000029405AFC310>, random_state=42)]},\n",
      "             scoring='recall_weighted', verbose=3)\n",
      "\n",
      "best_score_: 0.8137254901960784\n",
      "\n",
      "best_params_: {'dt_clf__random_state': 42, 'sel_per__percentile': 4, 'sel_per__score_func': functools.partial(<function mutual_info_classif at 0x0000029405AFC310>, random_state=42)}\n",
      "\n",
      " 1 sel_per_empty_rf_clf \n",
      "\n",
      "Fitting 5 folds for each of 10080 candidates, totalling 50400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 368 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1008 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1904 tasks      | elapsed:   25.8s\n",
      "[Parallel(n_jobs=-1)]: Done 3056 tasks      | elapsed:   43.6s\n",
      "[Parallel(n_jobs=-1)]: Done 4464 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6128 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 8048 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 10224 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 12656 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 15344 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 18288 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 21488 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 24944 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 28656 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 32624 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done 36848 tasks      | elapsed:  8.8min\n",
      "[Parallel(n_jobs=-1)]: Done 41328 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=-1)]: Done 46064 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done 50400 out of 50400 | elapsed: 11.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........... (step 1 of 2) Processing sel_per, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 2) Processing rf_clf, total=   0.0s\n",
      "\n",
      " GridSearchCV(estimator=Pipeline(steps=[('sel_per', SelectPercentile()),\n",
      "                                       ('rf_clf', RandomForestClassifier())],\n",
      "                                verbose=True),\n",
      "             n_jobs=-1,\n",
      "             param_grid={'rf_clf__bootstrap': [True, False],\n",
      "                         'rf_clf__max_depth': [16, 32, 64],\n",
      "                         'rf_clf__max_features': ['sqrt', 'log2'],\n",
      "                         'rf_clf__min_samples_leaf': [1, 2, 3, 4, 5],\n",
      "                         'rf_clf__min_samples_split': [2],\n",
      "                         'rf_clf__n_estimators': [2, 4, 6, 8, 10, 12, 14, 16],\n",
      "                         'rf_clf__n_jobs': [-1], 'rf_clf__random_state': [42],\n",
      "                         'sel_per__percentile': [2, 4, 8, 16, 32, 64, 100],\n",
      "                         'sel_per__score_func': [<function f_classif at 0x0000029405AD6430>,\n",
      "                                                 <function chi2 at 0x0000029405AD6790>,\n",
      "                                                 functools.partial(<function mutual_info_classif at 0x0000029405AFC310>, random_state=42)]},\n",
      "             scoring='recall_weighted', verbose=3)\n",
      "\n",
      "best_score_: 0.8725490196078433\n",
      "\n",
      "best_params_: {'rf_clf__bootstrap': False, 'rf_clf__max_depth': 16, 'rf_clf__max_features': 'sqrt', 'rf_clf__min_samples_leaf': 5, 'rf_clf__min_samples_split': 2, 'rf_clf__n_estimators': 10, 'rf_clf__n_jobs': -1, 'rf_clf__random_state': 42, 'sel_per__percentile': 32, 'sel_per__score_func': <function chi2 at 0x0000029405AD6790>}\n",
      "\n",
      " 2 sel_per_empty_ab_clf \n",
      "\n",
      "Fitting 5 folds for each of 1764 candidates, totalling 8820 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1008 tasks      | elapsed:   34.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1232 tasks      | elapsed:   56.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1520 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1872 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2352 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4272 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 5536 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 6144 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7264 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=-1)]: Done 8820 out of 8820 | elapsed:  8.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........... (step 1 of 2) Processing sel_per, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 2) Processing ab_clf, total=   0.5s\n",
      "\n",
      " GridSearchCV(estimator=Pipeline(steps=[('sel_per', SelectPercentile()),\n",
      "                                       ('ab_clf', AdaBoostClassifier())],\n",
      "                                verbose=True),\n",
      "             n_jobs=-1,\n",
      "             param_grid={'ab_clf__algorithm': ['SAMME', 'SAMME.R'],\n",
      "                         'ab_clf__base_estimator': [DecisionTreeClassifier(random_state=42),\n",
      "                                                    RandomForestClassifier(n_jobs=-1,\n",
      "                                                                           random_state=42),\n",
      "                                                    AdaBoostClassifier(random_state=42),\n",
      "                                                    SVC(random_state=42...\n",
      "                                                    GaussianNB()],\n",
      "                         'ab_clf__n_estimators': [8, 16, 24, 32, 40, 48, 56],\n",
      "                         'ab_clf__random_state': [42],\n",
      "                         'sel_per__percentile': [2, 4, 8, 16, 32, 64, 100],\n",
      "                         'sel_per__score_func': [<function f_classif at 0x0000029405AD6430>,\n",
      "                                                 <function chi2 at 0x0000029405AD6790>,\n",
      "                                                 functools.partial(<function mutual_info_classif at 0x0000029405AFC310>, random_state=42)]},\n",
      "             scoring='recall_weighted', verbose=3)\n",
      "\n",
      "best_score_: 0.8261437908496732\n",
      "\n",
      "best_params_: {'ab_clf__algorithm': 'SAMME', 'ab_clf__base_estimator': RandomForestClassifier(n_jobs=-1, random_state=42), 'ab_clf__n_estimators': 8, 'ab_clf__random_state': 42, 'sel_per__percentile': 8, 'sel_per__score_func': <function chi2 at 0x0000029405AD6790>}\n",
      "\n",
      " 3 sel_per_empty_kn_clf \n",
      "\n",
      "Fitting 5 folds for each of 7938 candidates, totalling 39690 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 368 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1008 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1904 tasks      | elapsed:   22.6s\n",
      "[Parallel(n_jobs=-1)]: Done 3056 tasks      | elapsed:   36.0s\n",
      "[Parallel(n_jobs=-1)]: Done 4464 tasks      | elapsed:   52.8s\n",
      "[Parallel(n_jobs=-1)]: Done 6128 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 8048 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 10224 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 12656 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 15344 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 18288 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 21488 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 24944 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 28656 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 32624 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 36848 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 39675 out of 39690 | elapsed:  7.6min remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 39690 out of 39690 | elapsed:  7.6min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........... (step 1 of 2) Processing sel_per, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 2) Processing kn_clf, total=   0.0s\n",
      "\n",
      " GridSearchCV(estimator=Pipeline(steps=[('sel_per', SelectPercentile()),\n",
      "                                       ('kn_clf', KNeighborsClassifier())],\n",
      "                                verbose=True),\n",
      "             n_jobs=-1,\n",
      "             param_grid={'kn_clf__algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
      "                         'kn_clf__leaf_size': [4, 8, 12, 16, 20, 24, 30],\n",
      "                         'kn_clf__n_jobs': [-1],\n",
      "                         'kn_clf__n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
      "                         'kn_clf__weights': ['uniform', 'distance'],\n",
      "                         'sel_per__percentile': [2, 4, 8, 16, 32, 64, 100],\n",
      "                         'sel_per__score_func': [<function f_classif at 0x0000029405AD6430>,\n",
      "                                                 <function chi2 at 0x0000029405AD6790>,\n",
      "                                                 functools.partial(<function mutual_info_classif at 0x0000029405AFC310>, random_state=42)]},\n",
      "             scoring='recall_weighted', verbose=3)\n",
      "\n",
      "best_score_: 0.8607843137254901\n",
      "\n",
      "best_params_: {'kn_clf__algorithm': 'ball_tree', 'kn_clf__leaf_size': 4, 'kn_clf__n_jobs': -1, 'kn_clf__n_neighbors': 2, 'kn_clf__weights': 'uniform', 'sel_per__percentile': 16, 'sel_per__score_func': <function f_classif at 0x0000029405AD6430>}\n",
      "\n",
      " 4 sel_per_empty_gnb_clf \n",
      "\n",
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of 105 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 105 out of 105 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........... (step 1 of 2) Processing sel_per, total=   0.0s\n",
      "[Pipeline] ........... (step 2 of 2) Processing gnb_clf, total=   0.0s\n",
      "\n",
      " GridSearchCV(estimator=Pipeline(steps=[('sel_per', SelectPercentile()),\n",
      "                                       ('gnb_clf', GaussianNB())],\n",
      "                                verbose=True),\n",
      "             n_jobs=-1,\n",
      "             param_grid={'sel_per__percentile': [2, 4, 8, 16, 32, 64, 100],\n",
      "                         'sel_per__score_func': [<function f_classif at 0x0000029405AD6430>,\n",
      "                                                 <function chi2 at 0x0000029405AD6790>,\n",
      "                                                 functools.partial(<function mutual_info_classif at 0x0000029405AFC310>, random_state=42)]},\n",
      "             scoring='recall_weighted', verbose=3)\n",
      "\n",
      "best_score_: 0.7673202614379084\n",
      "\n",
      "best_params_: {'sel_per__percentile': 32, 'sel_per__score_func': <function chi2 at 0x0000029405AD6790>}\n"
     ]
    }
   ],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "### Because the proliferation of features has led to overfit\n",
    "### (see gridsearch notebooks in the supplemental material folder),\n",
    "### I will remove the quantile flags, the outlier flags, the sign flags, and the cluster flags,\n",
    "### leaving the original base features (not already removed) and the ratio features.\n",
    "\n",
    "drop_lst = quant_flags_lst + out_flags_lst + sign_flags_lst + k_means_feats_lst\n",
    "keep_lst = [feat for feat in features_list[1:] if feat not in drop_lst]\n",
    "\n",
    "X_train_trimmed = X_train_scaled_imp_k[keep_lst]\n",
    "\n",
    "\n",
    "n_jobs = -1\n",
    "\n",
    "### Callables to pass into algorithms.\n",
    "mutual_info_classif_partial = partial(mutual_info_classif, random_state=42)\n",
    "DecisionTreeClassifier_partial = partial(DecisionTreeClassifier, random_state=42)\n",
    "RandomForestClassifier_partial = partial(RandomForestClassifier, random_state=42, n_jobs=n_jobs)\n",
    "AdaBoostClassifier_partial = partial(AdaBoostClassifier, random_state=42)\n",
    "svm_SVC_partial = partial(svm.SVC, random_state=42)\n",
    "KNeighborsClassifier_partial = partial(KNeighborsClassifier, n_jobs=n_jobs)\n",
    "\n",
    "selectors = {\n",
    "    'sel_per': {\n",
    "        'sel': SelectPercentile(),\n",
    "        'params': {\n",
    "            'sel_per__score_func': [f_classif, chi2, mutual_info_classif_partial],\n",
    "            'sel_per__percentile': [2, 4, 8, 16, 32, 64, 100]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "decomps = {\n",
    "    'empty' : None\n",
    "#     'fica': {\n",
    "#         'dec': FastICA(),\n",
    "#         'params': {\n",
    "#             'fica__algorithm': ['parallel', 'deflation'],\n",
    "#             'fica__fun': ['logcosh', 'exp', 'cube'],\n",
    "#             'fica__random_state': [42]\n",
    "#         }\n",
    "#     },\n",
    "#         'ipca': {\n",
    "#         'dec': IncrementalPCA(),\n",
    "#         'params': {\n",
    "#             ### defaults\n",
    "#         }\n",
    "#     },\n",
    "#     'kpca': {\n",
    "#         'dec': KernelPCA(),\n",
    "#         'params': {\n",
    "#             'kpca__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'cosine',\n",
    "#                              'precomputed'],\n",
    "#             'kpca__random_state': [42],\n",
    "#             'kpca__n_jobs': [n_jobs]\n",
    "#         }\n",
    "#     },\n",
    "    ### PCA kept throwing an error that the data contained nans, inf,\n",
    "    ### or too large dtypes, despite no nans, infs, or wrong types per\n",
    "    ### replications of the sklearn (and numpy) condition checks that threw the\n",
    "    ### errors (\"errors\" because using PCA threw the error from sklearn script,\n",
    "    ### but further testing showed that PCA's get_precision method may have\n",
    "    ### thrown it from a NumPy script (see cells below for tracing), and that\n",
    "    ### did not include dtype size).\n",
    "    ### Maybe (probably?) a problem with the transformed data handed off from\n",
    "    ### SelectPercentile, but I'm done testing. Just need to finish. Skip PCA.\n",
    "#     'pca': {\n",
    "#         'dec': PCA(),\n",
    "#         'params': {\n",
    "#             'pca__random_state': [42]\n",
    "#         }\n",
    "#     },\n",
    "#     'tsvd': {\n",
    "#         'dec': TruncatedSVD(),\n",
    "#         'params': {\n",
    "#             'tsvd__n_components': [2, 4, 8, 16, 32, 64, 128],\n",
    "#             'tsvd__algorithm': ['arpack', 'randomized'],\n",
    "#             'tsvd__random_state': [42]\n",
    "#         }\n",
    "#     }\n",
    "}\n",
    "\n",
    "classifiers = {\n",
    "    'dt_clf': {\n",
    "        'clf': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'dt_clf__random_state': [42]\n",
    "        }\n",
    "    },\n",
    "    'rf_clf': {\n",
    "        'clf': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'rf_clf__n_estimators': [2, 4, 6, 8, 10, 12, 14, 16],\n",
    "            'rf_clf__max_features': ['sqrt', 'log2'],\n",
    "            'rf_clf__max_depth': [16, 32, 64],\n",
    "            'rf_clf__min_samples_split': [2],\n",
    "            'rf_clf__min_samples_leaf': [1, 2, 3, 4, 5],\n",
    "            'rf_clf__bootstrap': [True, False],\n",
    "            'rf_clf__random_state': [42],\n",
    "            'rf_clf__n_jobs': [n_jobs]\n",
    "        }\n",
    "    },\n",
    "    'ab_clf': {\n",
    "        'clf': AdaBoostClassifier(),\n",
    "        'params': {\n",
    "            'ab_clf__base_estimator': [\n",
    "                DecisionTreeClassifier_partial(),\n",
    "                RandomForestClassifier_partial(),\n",
    "                AdaBoostClassifier_partial(),\n",
    "                svm_SVC_partial(),\n",
    "                KNeighborsClassifier_partial(),\n",
    "                GaussianNB()\n",
    "            ],\n",
    "            'ab_clf__n_estimators': [8, 16, 24, 32, 40, 48, 56],\n",
    "            'ab_clf__algorithm': ['SAMME', 'SAMME.R'],\n",
    "            'ab_clf__random_state': [42]\n",
    "        }\n",
    "    },\n",
    "    'kn_clf': {\n",
    "        'clf': KNeighborsClassifier(),\n",
    "        'params': {\n",
    "            'kn_clf__n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "            'kn_clf__weights': ['uniform', 'distance'],\n",
    "            'kn_clf__algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "            'kn_clf__leaf_size': [4, 8, 12, 16, 20, 24, 30],\n",
    "            'kn_clf__n_jobs': [n_jobs]\n",
    "        }\n",
    "    },\n",
    "    'gnb_clf': {\n",
    "        'clf': GaussianNB(),\n",
    "        'params': {\n",
    "            # Defaults\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "imp_gscvs_dict = {}\n",
    "imp_gscvs_dict['mixed_impute_trimmed'] \\\n",
    "    = search_em_all(X_train=X_train_trimmed, y_train=y_train_1d, selectors=selectors,\n",
    "                    decomps=decomps, classifiers=classifiers, pipe_verbose=True,\n",
    "                    scoring='recall_weighted', n_jobs=-1)\n",
    "### Can try with multiple datasets for comparison.\n",
    "# imp_gscvs_dict['other_set'] \\\n",
    "#     = search_em_all(X_train=X_train_other_set, y_train=y_train_1d, selectors=selectors,\n",
    "#                     decomps=decomps, classifiers=classifiers, pipe_verbose=True,\n",
    "#                     scoring='recall_weighted', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/imp_gscvs_dict4.pkl', 'wb') as file:\n",
    "    pickle.dump(obj=imp_gscvs_dict, file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mixed_impute_trimmed': {'sel_per_empty_dt_clf': GridSearchCV(estimator=Pipeline(steps=[('sel_per', SelectPercentile()),\n",
       "                                         ('dt_clf', DecisionTreeClassifier())],\n",
       "                                  verbose=True),\n",
       "               n_jobs=-1,\n",
       "               param_grid={'dt_clf__random_state': [42],\n",
       "                           'sel_per__percentile': [2, 4, 8, 16, 32, 64, 100],\n",
       "                           'sel_per__score_func': [<function f_classif at 0x0000029405AD6430>,\n",
       "                                                   <function chi2 at 0x0000029405AD6790>,\n",
       "                                                   functools.partial(<function mutual_info_classif at 0x0000029405AFC310>, random_state=42)]},\n",
       "               scoring='recall_weighted', verbose=3),\n",
       "  'sel_per_empty_rf_clf': GridSearchCV(estimator=Pipeline(steps=[('sel_per', SelectPercentile()),\n",
       "                                         ('rf_clf', RandomForestClassifier())],\n",
       "                                  verbose=True),\n",
       "               n_jobs=-1,\n",
       "               param_grid={'rf_clf__bootstrap': [True, False],\n",
       "                           'rf_clf__max_depth': [16, 32, 64],\n",
       "                           'rf_clf__max_features': ['sqrt', 'log2'],\n",
       "                           'rf_clf__min_samples_leaf': [1, 2, 3, 4, 5],\n",
       "                           'rf_clf__min_samples_split': [2],\n",
       "                           'rf_clf__n_estimators': [2, 4, 6, 8, 10, 12, 14, 16],\n",
       "                           'rf_clf__n_jobs': [-1], 'rf_clf__random_state': [42],\n",
       "                           'sel_per__percentile': [2, 4, 8, 16, 32, 64, 100],\n",
       "                           'sel_per__score_func': [<function f_classif at 0x0000029405AD6430>,\n",
       "                                                   <function chi2 at 0x0000029405AD6790>,\n",
       "                                                   functools.partial(<function mutual_info_classif at 0x0000029405AFC310>, random_state=42)]},\n",
       "               scoring='recall_weighted', verbose=3),\n",
       "  'sel_per_empty_ab_clf': GridSearchCV(estimator=Pipeline(steps=[('sel_per', SelectPercentile()),\n",
       "                                         ('ab_clf', AdaBoostClassifier())],\n",
       "                                  verbose=True),\n",
       "               n_jobs=-1,\n",
       "               param_grid={'ab_clf__algorithm': ['SAMME', 'SAMME.R'],\n",
       "                           'ab_clf__base_estimator': [DecisionTreeClassifier(random_state=42),\n",
       "                                                      RandomForestClassifier(n_jobs=-1,\n",
       "                                                                             random_state=42),\n",
       "                                                      AdaBoostClassifier(random_state=42),\n",
       "                                                      SVC(random_state=42...\n",
       "                                                      GaussianNB()],\n",
       "                           'ab_clf__n_estimators': [8, 16, 24, 32, 40, 48, 56],\n",
       "                           'ab_clf__random_state': [42],\n",
       "                           'sel_per__percentile': [2, 4, 8, 16, 32, 64, 100],\n",
       "                           'sel_per__score_func': [<function f_classif at 0x0000029405AD6430>,\n",
       "                                                   <function chi2 at 0x0000029405AD6790>,\n",
       "                                                   functools.partial(<function mutual_info_classif at 0x0000029405AFC310>, random_state=42)]},\n",
       "               scoring='recall_weighted', verbose=3),\n",
       "  'sel_per_empty_kn_clf': GridSearchCV(estimator=Pipeline(steps=[('sel_per', SelectPercentile()),\n",
       "                                         ('kn_clf', KNeighborsClassifier())],\n",
       "                                  verbose=True),\n",
       "               n_jobs=-1,\n",
       "               param_grid={'kn_clf__algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
       "                           'kn_clf__leaf_size': [4, 8, 12, 16, 20, 24, 30],\n",
       "                           'kn_clf__n_jobs': [-1],\n",
       "                           'kn_clf__n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       "                           'kn_clf__weights': ['uniform', 'distance'],\n",
       "                           'sel_per__percentile': [2, 4, 8, 16, 32, 64, 100],\n",
       "                           'sel_per__score_func': [<function f_classif at 0x0000029405AD6430>,\n",
       "                                                   <function chi2 at 0x0000029405AD6790>,\n",
       "                                                   functools.partial(<function mutual_info_classif at 0x0000029405AFC310>, random_state=42)]},\n",
       "               scoring='recall_weighted', verbose=3),\n",
       "  'sel_per_empty_gnb_clf': GridSearchCV(estimator=Pipeline(steps=[('sel_per', SelectPercentile()),\n",
       "                                         ('gnb_clf', GaussianNB())],\n",
       "                                  verbose=True),\n",
       "               n_jobs=-1,\n",
       "               param_grid={'sel_per__percentile': [2, 4, 8, 16, 32, 64, 100],\n",
       "                           'sel_per__score_func': [<function f_classif at 0x0000029405AD6430>,\n",
       "                                                   <function chi2 at 0x0000029405AD6790>,\n",
       "                                                   functools.partial(<function mutual_info_classif at 0x0000029405AFC310>, random_state=42)]},\n",
       "               scoring='recall_weighted', verbose=3)}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_gscvs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sel_per_empty_dt_clf \n",
      "\n",
      "Best score:\n",
      " 0.8137254901960784 \n",
      "\n",
      "Best estimator:\n",
      " Pipeline(steps=[('sel_per',\n",
      "                 SelectPercentile(percentile=4,\n",
      "                                  score_func=functools.partial(<function mutual_info_classif at 0x0000029405AFC310>, random_state=42))),\n",
      "                ('dt_clf', DecisionTreeClassifier(random_state=42))],\n",
      "         verbose=True) \n",
      "\n",
      "[Pipeline] ........... (step 1 of 2) Processing sel_per, total=   0.8s\n",
      "[Pipeline] ............ (step 2 of 2) Processing dt_clf, total=   0.0s\n",
      "Confusion matrix:\n",
      " [[31  3]\n",
      " [ 1  2]] \n",
      "\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.96875, 0.4    ]), array([0.91176471, 0.66666667]), array([0.93939394, 0.5       ]), array([34,  3], dtype=int64)) \n",
      "\n",
      "Custom F beta using nonPOI precision and POI recall:\n",
      " 0.7898089171974522 \n",
      "\n",
      "\n",
      "\n",
      "sel_per_empty_rf_clf \n",
      "\n",
      "Best score:\n",
      " 0.8725490196078433 \n",
      "\n",
      "Best estimator:\n",
      " Pipeline(steps=[('sel_per',\n",
      "                 SelectPercentile(percentile=32,\n",
      "                                  score_func=<function chi2 at 0x0000029405AD6790>)),\n",
      "                ('rf_clf',\n",
      "                 RandomForestClassifier(bootstrap=False, max_depth=16,\n",
      "                                        max_features='sqrt', min_samples_leaf=5,\n",
      "                                        n_estimators=10, n_jobs=-1,\n",
      "                                        random_state=42))],\n",
      "         verbose=True) \n",
      "\n",
      "[Pipeline] ........... (step 1 of 2) Processing sel_per, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 2) Processing rf_clf, total=   0.0s\n",
      "Confusion matrix:\n",
      " [[33  1]\n",
      " [ 2  1]] \n",
      "\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.94285714, 0.5       ]), array([0.97058824, 0.33333333]), array([0.95652174, 0.4       ]), array([34,  3], dtype=int64)) \n",
      "\n",
      "Custom F beta using nonPOI precision and POI recall:\n",
      " 0.49253731343283585 \n",
      "\n",
      "\n",
      "\n",
      "sel_per_empty_ab_clf \n",
      "\n",
      "Best score:\n",
      " 0.8261437908496732 \n",
      "\n",
      "Best estimator:\n",
      " Pipeline(steps=[('sel_per',\n",
      "                 SelectPercentile(percentile=8,\n",
      "                                  score_func=<function chi2 at 0x0000029405AD6790>)),\n",
      "                ('ab_clf',\n",
      "                 AdaBoostClassifier(algorithm='SAMME',\n",
      "                                    base_estimator=RandomForestClassifier(n_jobs=-1,\n",
      "                                                                          random_state=42),\n",
      "                                    n_estimators=8, random_state=42))],\n",
      "         verbose=True) \n",
      "\n",
      "[Pipeline] ........... (step 1 of 2) Processing sel_per, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 2) Processing ab_clf, total=   0.1s\n",
      "Confusion matrix:\n",
      " [[33  1]\n",
      " [ 2  1]] \n",
      "\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.94285714, 0.5       ]), array([0.97058824, 0.33333333]), array([0.95652174, 0.4       ]), array([34,  3], dtype=int64)) \n",
      "\n",
      "Custom F beta using nonPOI precision and POI recall:\n",
      " 0.49253731343283585 \n",
      "\n",
      "\n",
      "\n",
      "sel_per_empty_kn_clf \n",
      "\n",
      "Best score:\n",
      " 0.8607843137254901 \n",
      "\n",
      "Best estimator:\n",
      " Pipeline(steps=[('sel_per', SelectPercentile(percentile=16)),\n",
      "                ('kn_clf',\n",
      "                 KNeighborsClassifier(algorithm='ball_tree', leaf_size=4,\n",
      "                                      n_jobs=-1, n_neighbors=2))],\n",
      "         verbose=True) \n",
      "\n",
      "[Pipeline] ........... (step 1 of 2) Processing sel_per, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 2) Processing kn_clf, total=   0.0s\n",
      "Confusion matrix:\n",
      " [[33  1]\n",
      " [ 3  0]] \n",
      "\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.91666667, 0.        ]), array([0.97058824, 0.        ]), array([0.94285714, 0.        ]), array([34,  3], dtype=int64)) \n",
      "\n",
      "Custom F beta using nonPOI precision and POI recall:\n",
      " 0.0 \n",
      "\n",
      "\n",
      "\n",
      "sel_per_empty_gnb_clf \n",
      "\n",
      "Best score:\n",
      " 0.7673202614379084 \n",
      "\n",
      "Best estimator:\n",
      " Pipeline(steps=[('sel_per',\n",
      "                 SelectPercentile(percentile=32,\n",
      "                                  score_func=<function chi2 at 0x0000029405AD6790>)),\n",
      "                ('gnb_clf', GaussianNB())],\n",
      "         verbose=True) \n",
      "\n",
      "[Pipeline] ........... (step 1 of 2) Processing sel_per, total=   0.0s\n",
      "[Pipeline] ........... (step 2 of 2) Processing gnb_clf, total=   0.0s\n",
      "Confusion matrix:\n",
      " [[21 13]\n",
      " [ 0  3]] \n",
      "\n",
      "Precision, recall, f beta score, support:\n",
      " (array([1.    , 0.1875]), array([0.61764706, 1.        ]), array([0.76363636, 0.31578947]), array([34,  3], dtype=int64)) \n",
      "\n",
      "Custom F beta using nonPOI precision and POI recall:\n",
      " 1.0 \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaleb\\anaconda3\\envs\\py3\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:114: UserWarning: Features [211 220 225 230 233 234 235 236 237 241 247 252 255 257 321 328 335 336\n",
      " 337 342 347 350 351 352 353 354 358 364 366 367 368 369 370 375 382 383\n",
      " 386 387 388] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx,\n",
      "C:\\Users\\kaleb\\anaconda3\\envs\\py3\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:116: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "get_f = lambda precision, recall: 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "for name, gscv in imp_gscvs_dict['mixed_impute_trimmed'].items():\n",
    "    print(name, '\\n')\n",
    "    print('Best score:\\n', gscv.best_score_, '\\n')\n",
    "    print('Best estimator:\\n', gscv.best_estimator_, '\\n')\n",
    "    clf = gscv.best_estimator_.fit(X=X_train_scaled_imp_k, y=y_train_1d)\n",
    "    pred = clf.predict(X_test_scaled_imp_k)\n",
    "    conf = confusion_matrix(y_true=y_test_1d, y_pred=pred)\n",
    "    print('Confusion matrix:\\n', conf, '\\n')\n",
    "    prf = precision_recall_fscore_support(y_true=y_test_1d, y_pred=pred)\n",
    "    print('Precision, recall, f beta score, support:\\n', prf, '\\n')\n",
    "    print('Custom F beta using nonPOI precision and POI recall:\\n', get_f(prf[0][0], prf[1][1]), '\\n')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "features_list = keep_lst\n",
    "my_dataset = df.T.to_dict()\n",
    "clf = imp_gscvs_dict['mixed_impute']['<name>_clf']\n",
    "\n",
    "CLF_PICKLE_FILENAME = \"data/my_classifier.pkl\"\n",
    "DATASET_PICKLE_FILENAME = \"data/my_dataset.pkl\"\n",
    "FEATURE_LIST_FILENAME = \"data/my_feature_list.pkl\"\n",
    "\n",
    "with open(CLF_PICKLE_FILENAME, \"w\") as clf_outfile:\n",
    "    pickle.dump(clf, clf_outfile)\n",
    "with open(DATASET_PICKLE_FILENAME, \"w\") as dataset_outfile:\n",
    "    pickle.dump(my_dataset, dataset_outfile)\n",
    "with open(FEATURE_LIST_FILENAME, \"w\") as featurelist_outfile:\n",
    "    pickle.dump(features_list, featurelist_outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tester import dump_classifier_and_data\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "import tester"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3] *",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
