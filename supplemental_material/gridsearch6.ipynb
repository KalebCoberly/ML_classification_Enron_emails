{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "# from time import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import math\n",
    "from scipy import stats\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "\n",
    "from sklearn.feature_selection import SelectPercentile, SelectFromModel, f_classif, mutual_info_classif, chi2,\\\n",
    "                                        SelectFpr, SelectFdr, RFECV\n",
    "from sklearn.decomposition import FastICA, IncrementalPCA, KernelPCA, PCA, TruncatedSVD\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "### My imports\n",
    "sys.path.append('tools/')\n",
    "from dos2unix import crlf_to_lf # Borrowed and modified from multiple sources.\n",
    "from train_test import run_skl, get_base_perfs, search_em_all\n",
    "from feature_engineering import set_all_ratios, quant_flag_all, out_flag_all, flag_signs, add_k_means_n\n",
    "\n",
    "### Udacity imports\n",
    "# from feature_format import featureFormat, targetFeatureSplit # (may be modified)\n",
    "# from tester import dump_classifier_and_data\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/final_project_dataset.pkl saved as data/final_project_dataset_unix.pkl in 6705 bytes.\n"
     ]
    }
   ],
   "source": [
    "##########################################################################################\n",
    "### Load the dictionary containing the dataset, and clean it up.\n",
    "### Make the dict a dataframe because they're easier to work with.\n",
    "data_df = None #pd.DataFrame()\n",
    "fp = crlf_to_lf(f_in_path='data/final_project_dataset.pkl')\n",
    "with open(fp, 'rb') as data_file:\n",
    "    data_df = pd.DataFrame(pickle.load(data_file)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "### Task 1: Clean up and select what features and subsets *not* to use.\n",
    "### (Further feature selection will happen after feature engineering.)\n",
    "    \n",
    "### Drop email_address since it's a signature.\n",
    "data_df.drop(columns='email_address', inplace=True)\n",
    "### Drop the TOTAL row.\n",
    "data_df.drop(labels=['TOTAL', 'THE TRAVEL AGENCY IN THE PARK'], inplace=True)\n",
    "\n",
    "### Handle missing values here.\n",
    "### Replacing 'NaN' with None had a weird result in which values from some\n",
    "### rows were copied into the missing values of neighboring rows. No idea why.\n",
    "### Using np.nan did not have that result as far as I can tell.\n",
    "### But it is a float missing value and thus casts the column as float,\n",
    "### or as object when other values are not floats.\n",
    "data_df.replace(to_replace='NaN', value=np.nan, inplace=True)\n",
    "\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\"\n",
    "###    (if using featureFormat(), which I don't).\n",
    "\n",
    "### All units are in USD.\n",
    "fin_features = ['salary', 'bonus', 'long_term_incentive', 'deferred_income', 'deferral_payments',\n",
    "                'loan_advances', 'other', 'expenses', 'director_fees', 'total_payments',\n",
    "                'exercised_stock_options', 'restricted_stock', 'restricted_stock_deferred', 'total_stock_value']\n",
    "pay_features = fin_features[:10]\n",
    "stock_features = fin_features[10:]\n",
    "    \n",
    "### Units are number of emails messages;\n",
    "email_features = ['to_messages', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi',\n",
    "                  'shared_receipt_with_poi']\n",
    "\n",
    "### Boolean, represented as integer.\n",
    "POI_label = ['poi']\n",
    "\n",
    "### The first feature must be \"poi\" if using featureFormat().\n",
    "features_list = POI_label + fin_features + email_features\n",
    "\n",
    "### Imputation recasts as float, but as object if left as bool, so set it to int for now.\n",
    "data_df['poi'] = data_df['poi'].astype(dtype=int)\n",
    "\n",
    "### Belfer's financial data is shifted one column to the right.\n",
    "### Shift it one to the left, financial data only.\n",
    "### Make total_stock_value np.nan for consistency until imputation, but could be 0.\n",
    "### May remove this row for so many NaNs, but fix it now anyway.\n",
    "data_df.loc[data_df.index == 'BELFER ROBERT', fin_features] \\\n",
    "    = data_df.loc[data_df.index == 'BELFER ROBERT', fin_features].shift(periods=-1, axis='columns',\n",
    "                                                                        fill_value=np.nan)\n",
    "\n",
    "### Bhatnagar's financial data is shifted one to the left.\n",
    "### Shift it one to the right, financial data only.\n",
    "### Make salary np.nan.\n",
    "data_df.loc[data_df.index == 'BHATNAGAR SANJAY', fin_features] \\\n",
    "    = data_df.loc[data_df.index == 'BHATNAGAR SANJAY', fin_features].shift(periods=1, axis='columns',\n",
    "                                                                           fill_value=np.nan)\n",
    "\n",
    "### Set totals to sum of values where any values are not NaN.\n",
    "### i.e. don't make 0 totals NaN, even though some NaN values may be included.\n",
    "### Makes these rows consistent with other rows that include NaNs and numbers yet have a nonNaN total.\n",
    "data_df.loc[~(data_df[pay_features].isna().all(axis='columns')), 'total_payments'] \\\n",
    "    = data_df[pay_features[:-1]].sum(axis='columns')\n",
    "data_df.loc[~(data_df[stock_features].isna().all(axis='columns')), 'total_stock_value'] \\\n",
    "    = data_df[stock_features[:-1]].sum(axis='columns')\n",
    "\n",
    "### Add one to Glisan's to_message to at least equal shared_receipt_with_poi.\n",
    "data_df.loc['GLISAN JR BEN F', 'to_messages'] = 874\n",
    "\n",
    "### Drop features that are too sparse.\n",
    "drop_feats_lst = ['loan_advances']\n",
    "data_df.drop(columns=drop_feats_lst, inplace=True)\n",
    "fin_features = [feat for feat in fin_features if feat not in drop_feats_lst]\n",
    "pay_features = [feat for feat in pay_features if feat not in drop_feats_lst]\n",
    "stock_features = [feat for feat in stock_features if feat not in drop_feats_lst]\n",
    "email_features = [feat for feat in email_features if feat not in drop_feats_lst]\n",
    "features_list = [feat for feat in features_list if feat not in drop_feats_lst]\n",
    "\n",
    "### Removed 'email' as signature upon loading.\n",
    "\n",
    "### Drop persons who have NaN payment totals or NaN stock totals or NaN to_messages or NaN from_messages,\n",
    "### and are missing 70% of their values.\n",
    "### (Already made sure that all totals are not NaN if they have subvalues.)\n",
    "nan_limit = 0.7 * len(data_df.columns)\n",
    "sparse_records_idx_arr = \\\n",
    "    data_df.loc[data_df['total_payments'].isna() \\\n",
    "                | data_df['total_stock_value'].isna() \\\n",
    "                | data_df['to_messages'].isna() \\\n",
    "                | data_df['from_messages'].isna()]\\\n",
    "           .loc[data_df.isna().sum(axis='columns') > nan_limit]\\\n",
    "           .index.values\n",
    "data_df.drop(labels=sparse_records_idx_arr, inplace=True)\n",
    "\n",
    "### This leaves 123 records over 19 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline model performance metrics:\n",
      "\n",
      "DecisionTreeClassifier()\n",
      "Training time: 0.002 s\n",
      "Prediction time: 0.001 s\n",
      "Confusion matrix:\n",
      " [[19  3]\n",
      " [ 3  1]]\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.86363636, 0.25      ]), array([0.86363636, 0.25      ]), array([0.86363636, 0.25      ]), array([22,  4], dtype=int64))\n",
      "RandomForestClassifier()\n",
      "Training time: 0.093 s\n",
      "Prediction time: 0.007 s\n",
      "Confusion matrix:\n",
      " [[22  0]\n",
      " [ 4  0]]\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.84615385, 0.        ]), array([1., 0.]), array([0.91666667, 0.        ]), array([22,  4], dtype=int64))\n",
      "AdaBoostClassifier()\n",
      "Training time: 0.049 s\n",
      "Prediction time: 0.006 s\n",
      "Confusion matrix:\n",
      " [[20  2]\n",
      " [ 3  1]]\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.86956522, 0.33333333]), array([0.90909091, 0.25      ]), array([0.88888889, 0.28571429]), array([22,  4], dtype=int64))\n",
      "KNeighborsClassifier()\n",
      "Training time: 0.002 s\n",
      "Prediction time: 0.002 s\n",
      "Confusion matrix:\n",
      " [[22  0]\n",
      " [ 4  0]]\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.84615385, 0.        ]), array([1., 0.]), array([0.91666667, 0.        ]), array([22,  4], dtype=int64))\n",
      "GaussianNB()\n",
      "Training time: 0.001 s\n",
      "Prediction time: 0.001 s\n",
      "Confusion matrix:\n",
      " [[16  6]\n",
      " [ 1  3]]\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.94117647, 0.33333333]), array([0.72727273, 0.75      ]), array([0.82051282, 0.46153846]), array([22,  4], dtype=int64))\n",
      "SVC()\n",
      "Training time: 0.001 s\n",
      "Prediction time: 0.002 s\n",
      "Confusion matrix:\n",
      " [[22  0]\n",
      " [ 4  0]]\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.84615385, 0.        ]), array([1., 0.]), array([0.91666667, 0.        ]), array([22,  4], dtype=int64))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaleb\\anaconda3\\envs\\py3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kaleb\\anaconda3\\envs\\py3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kaleb\\anaconda3\\envs\\py3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "### Make a quick baseline model for comparison.\n",
    "\n",
    "### Alphabetize index before split for Udacity compatibility because that's what they'll do.\n",
    "### I knew this, but missed it until the end. I'd decided from the start not to use their deprecated\n",
    "### scripts based on dictionaries, specifically feature_format, and wrote my own using pandas. \n",
    "### The rest of my cleaning and engineering are based on a different split.\n",
    "### My mistake. Lesson learned: pay closer attention to what the legacy code does, especially \n",
    "### expected input/output structures.\n",
    "data_df.sort_index(inplace=True)\n",
    "\n",
    "### Impute with 0.\n",
    "imp_0 = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0, copy=False)\n",
    "imp_0 = imp_0.fit(X=data_df)\n",
    "data_imp0_df = pd.DataFrame(data=imp_0.transform(X=data_df), columns=data_df.columns, index=data_df.index)\n",
    "\n",
    "### Split now for baseline model, but also before further processing, outlier removal, scaling, engineering,\n",
    "### or else test set info leaks into training set.\n",
    "### Even imputation could if using multivariate imputation or median.\n",
    "### Decision on how to treat the data should not be influenced by test set either.\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_imp0_df[features_list[1:]], data_imp0_df[['poi']],\n",
    "                                                    test_size=.3, random_state=42)\n",
    "### Some algorithms want 1D y data.\n",
    "y_train_1d = np.ravel(y_train.astype(bool))\n",
    "y_test_1d = np.ravel(y_test.astype(bool))\n",
    "\n",
    "### Split train set again for a baseline model that won't touch the final test set.\n",
    "X_train_base, X_test_base, y_train_base, y_test_base \\\n",
    "    = train_test_split(X_train, y_train, test_size=.3, random_state=42)\n",
    "y_train_1d_base = np.ravel(y_train_base.astype(bool))\n",
    "y_test_1d_base = np.ravel(y_test_base.astype(bool))\n",
    "\n",
    "### For metrics.\n",
    "ordered_cols_lst = ['nonPOI_prec', 'POI_prec', 'nonPOI_rec', 'POI_rec', 'nonPOI_f', 'POI_f', 'nonPOI_sup',\n",
    "                    'POI_sup', 't_neg', 'f_neg', 'f_pos', 't_pos', 'train_t', 'predict_t', 'model']\n",
    "base_perf_df = pd.DataFrame(columns=ordered_cols_lst)\n",
    "\n",
    "clf_dict = {'dt_clf': DecisionTreeClassifier, 'rf_clf': RandomForestClassifier, 'ab_clf': AdaBoostClassifier,\n",
    "            'kn_clf': KNeighborsClassifier, 'gnb_clf': GaussianNB, 'svc_clf': svm.SVC}\n",
    "\n",
    "print('\\nBaseline model performance metrics:\\n')\n",
    "for key, method in clf_dict.items():\n",
    "    _, _, _, _, perf_sr = run_skl(method=method, X_train=X_train_base,\n",
    "                                  y_train=y_train_1d_base,\n",
    "                                  X_test=X_test_base,\n",
    "                                  y_test=y_test_1d_base,\n",
    "                                  perf_series=key)\n",
    "    base_perf_df = base_perf_df.append(perf_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "### Task 2: Remove/handle outliers\n",
    "\n",
    "### Dropped ['TOTAL', 'THE TRAVEL AGENCY IN THE PARK'] row upon loading.\n",
    "\n",
    "### Drop features that are too sparse.\n",
    "### Drop 'other' because it's ill-defined and seems overly represented within important features. The nebulous nature of it seems like a good fit for fraud, but high gross 'other' amounts are more correlated with nonPOIs than POIs if anything.\n",
    "drop_feats_lst = ['director_fees', 'restricted_stock_deferred', 'other']\n",
    "\n",
    "X_train.drop(columns=drop_feats_lst, inplace=True)\n",
    "X_test.drop(columns=drop_feats_lst, inplace=True)\n",
    "data_df.drop(columns=drop_feats_lst, inplace=True)\n",
    "\n",
    "fin_features = [feat for feat in fin_features if feat not in drop_feats_lst]\n",
    "pay_features = [feat for feat in pay_features if feat not in drop_feats_lst]\n",
    "stock_features = [feat for feat in stock_features if feat not in drop_feats_lst]\n",
    "email_features = [feat for feat in email_features if feat not in drop_feats_lst]\n",
    "features_list = [feat for feat in features_list if feat not in drop_feats_lst]\n",
    "del drop_feats_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Don't drop records now because it will mess up the split for Udacity.\n",
    "### Could drop earlier and resplit, but I've already done a lot of EDA behind the scenes.\n",
    "### NaN his financials.\n",
    "X_train.loc[['POWERS WILLIAM'], pay_features] = np.nan\n",
    "data_df.loc[['POWERS WILLIAM'], pay_features] = np.nan\n",
    "\n",
    "### Bivariate linear regression of the ratios between to/from/shared with POIs and\n",
    "### total to and from messages revealed that top coding to_messages and from_messages\n",
    "### may slightly aid nonPOI precision.\n",
    "### Only top coding the training set in order to bias the model,\n",
    "### since I am less concerned with accuracy than I am with POI recall,\n",
    "### and by extension, nonPOI precision.\n",
    "X_train['to_messages'] = X_train['to_messages'].apply(lambda x: x if x < 12000 or np.isnan(x) else 12000)\n",
    "X_train['from_messages'] = X_train['from_messages'].apply(lambda x: x if x < 8000 or np.isnan(x) else 8000)\n",
    "data_df.loc[X_train.index]['to_messages'] \\\n",
    "    = data_df.loc[X_train.index]['to_messages'].apply(lambda x: x if x < 12000 or np.isnan(x) else 12000)\n",
    "data_df.loc[X_train.index]['from_messages'] \\\n",
    "    = data_df.loc[X_train.index]['from_messages'].apply(lambda x: x if x < 8000 or np.isnan(x) else 8000)\n",
    "\n",
    "### Not sure whether top coding these will really help or hinder, if anything at all.\n",
    "### But, it appears to potentially aid POI recall in some cases\n",
    "### when comparing payments to totals, and it's more in line with best practices.\n",
    "### Only really affects Frevert.\n",
    "top = X_train['total_payments'].dropna().sort_values()[-2]\n",
    "X_train['total_payments'] = X_train['total_payments'].apply(lambda x : x if x < top or np.isnan(x) else top)\n",
    "data_df.loc[X_train.index]['total_payments'] \\\n",
    "    = data_df.loc[X_train.index]['total_payments'].apply(lambda x : x if x < top or np.isnan(x) else top)\n",
    "\n",
    "top = X_train['long_term_incentive'].dropna().sort_values()[-2]\n",
    "X_train['long_term_incentive'] = \\\n",
    "    X_train['long_term_incentive'].apply(lambda x : x if x < top or np.isnan(x) else top)\n",
    "data_df.loc[X_train.index]['long_term_incentive'] \\\n",
    "    = data_df.loc[X_train.index]['long_term_incentive'].apply(lambda x : x if x < top or np.isnan(x) else top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Same story as Powers, NaN all of Belfer instead of simply dropping.\n",
    "X_train.loc['BELFER ROBERT'] = np.nan\n",
    "# belfers_poi = data_df.loc['BELFER ROBERT']['poi']\n",
    "data_df.loc['BELFER ROBERT', features_list[1:]]= np.nan\n",
    "# data_df.loc['BELFER ROBERT']['poi'] = belfers_poi\n",
    "\n",
    "### After look at distributions of ratios of features, more top/bottom coding. ###\n",
    "\n",
    "### Nan Bannantine's salary, and bottom code salary.\n",
    "X_train.loc['BANNANTINE JAMES M', 'salary'] = np.nan\n",
    "data_df.loc['BANNANTINE JAMES M', 'salary'] = np.nan\n",
    "bottom = X_train['salary'].dropna().sort_values(ascending=False)[-2]\n",
    "X_train['salary'] = X_train['salary'].apply(lambda x : x if x > bottom or np.isnan(x) else bottom)\n",
    "data_df.loc[X_train.index]['salary'] \\\n",
    "    = data_df.loc[X_train.index]['salary'].apply(lambda x : x if x > bottom or np.isnan(x) else bottom)\n",
    "\n",
    "### These two only have one, very low payment value.\n",
    "# X_train.loc[['HAYES ROBERT E', 'HAUG DAVID L'], pay_features] = np.nan\n",
    "# data_df.loc[['HAYES ROBERT E', 'HAUG DAVID L'], pay_features] = np.nan\n",
    "X_train.loc[['HAYES ROBERT E'], pay_features] = np.nan\n",
    "data_df.loc[['HAYES ROBERT E'], pay_features] = np.nan\n",
    "\n",
    "### Top code deferred_income.\n",
    "top = X_train['deferred_income'].dropna().sort_values(ascending=True)[-3]\n",
    "X_train['deferred_income'] = X_train['deferred_income'].apply(lambda x : x if x < top or np.isnan(x) else top)\n",
    "data_df.loc[X_train.index]['deferred_income'] = \\\n",
    "    data_df.loc[X_train.index]['deferred_income'].apply(lambda x : x if x < top or np.isnan(x) else top)\n",
    "del top\n",
    "del bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "### Task 3: Create new feature(s)\n",
    "\n",
    "\n",
    "### Start with all ratios, within respective subspaces (fin:fin, e:e).\n",
    "### Add financial ratios within subspaces to data sets.\n",
    "pay_feats_divby_df = set_all_ratios(df=X_train, denoms=pay_features, numers=pay_features)\n",
    "stock_feats_divby_df = set_all_ratios(df=X_train, denoms=stock_features, numers=stock_features)\n",
    "\n",
    "### Only plausible email ratios (all reciprocals still, to get the 0s to infs):\n",
    "to_lst = ['to_messages', 'from_poi_to_this_person', 'shared_receipt_with_poi']\n",
    "from_lst = ['from_messages', 'from_this_person_to_poi']\n",
    "email_to_divby_df = set_all_ratios(df=X_train, denoms=to_lst, numers=to_lst)\n",
    "email_from_divby_df = set_all_ratios(df=X_train, denoms=from_lst, numers=from_lst)\n",
    "\n",
    "X_train = pd.concat(objs=[X_train, pay_feats_divby_df, stock_feats_divby_df, email_to_divby_df,\n",
    "                          email_from_divby_df], axis=1)\n",
    "\n",
    "### Do for test set.\n",
    "pay_feats_divby_df = set_all_ratios(df=X_test, denoms=pay_features, numers=pay_features)\n",
    "stock_feats_divby_df = set_all_ratios(df=X_test, denoms=stock_features, numers=stock_features)\n",
    "email_to_divby_df = set_all_ratios(df=X_test, denoms=to_lst, numers=to_lst)\n",
    "email_from_divby_df = set_all_ratios(df=X_test, denoms=from_lst, numers=from_lst)\n",
    "X_test = pd.concat(objs=[X_test, pay_feats_divby_df, stock_feats_divby_df, email_to_divby_df,\n",
    "                         email_from_divby_df], axis=1)\n",
    "\n",
    "### Do for full set.\n",
    "pay_feats_divby_df = set_all_ratios(df=data_df, denoms=pay_features, numers=pay_features)\n",
    "stock_feats_divby_df = set_all_ratios(df=data_df, denoms=stock_features, numers=stock_features)\n",
    "email_to_divby_df = set_all_ratios(df=data_df, denoms=to_lst, numers=to_lst)\n",
    "email_from_divby_df = set_all_ratios(df=data_df, denoms=from_lst, numers=from_lst)\n",
    "data_df = pd.concat(objs=[data_df, pay_feats_divby_df, stock_feats_divby_df, email_to_divby_df,\n",
    "                          email_from_divby_df], axis=1)\n",
    "del to_lst\n",
    "del from_lst\n",
    "\n",
    "### Set all np.inf to np.nan.\n",
    "X_train = X_train.apply(func=(lambda col: col.apply(func=(lambda x: np.nan if abs(x) == abs(np.inf) else x))))\n",
    "X_test = X_test.apply(func=(lambda col: col.apply(func=(lambda x: np.nan if abs(x) == abs(np.inf) else x))))\n",
    "data_df = data_df.apply(func=(lambda col: col.apply(func=(lambda x: np.nan if abs(x) == abs(np.inf) else x))))\n",
    "\n",
    "### Remove all features containing less than 30% training observations.\n",
    "drop_lst = list(X_train.count().loc[X_train.count() < .3 * len(X_train.index)].index)\n",
    "X_train.drop(columns=drop_lst, inplace=True)\n",
    "X_test.drop(columns=drop_lst, inplace=True)\n",
    "data_df.drop(columns=drop_lst, inplace=True)\n",
    "\n",
    "pay_feats_divby_lst = [feat for feat in list(pay_feats_divby_df.columns) if not feat in drop_lst]\n",
    "stock_feats_divby_lst = [feat for feat in list(stock_feats_divby_df.columns) if not feat in drop_lst]\n",
    "email_feats_divby_lst = [feat for feat in list(email_to_divby_df.columns) if not feat in drop_lst] \\\n",
    "                        + [feat for feat in list(email_from_divby_df.columns) if not feat in drop_lst]\n",
    "fin_features = [feat for feat in fin_features if feat not in drop_lst] + pay_feats_divby_lst \\\n",
    "    + stock_feats_divby_lst\n",
    "pay_features = [feat for feat in pay_features if feat not in drop_lst]\n",
    "stock_features = [feat for feat in stock_features if feat not in drop_lst]\n",
    "email_features = [feat for feat in email_features if feat not in drop_lst] + email_feats_divby_lst\n",
    "features_list = [feat for feat in features_list if feat not in drop_lst] + pay_feats_divby_lst \\\n",
    "    + stock_feats_divby_lst + email_feats_divby_lst\n",
    "del drop_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create features that flag mambership in various quantiles, outliership, and x > 0.\n",
    "### Use multiple quantiles: quartiles, quintiles, and deciles.\n",
    "### Retain np.nans.\n",
    "\n",
    "to_flag_lst = fin_features + email_features\n",
    "\n",
    "### Could write a function, but I'll just paste and edit.\n",
    "### Flag train set.\n",
    "fin_quant_flags_df = quant_flag_all(df=X_train[fin_features], quant_df=X_train[fin_features])\n",
    "email_quant_flags_df = quant_flag_all(df=X_train[email_features], quant_df=X_train[email_features])\n",
    "fin_out_flags_df = out_flag_all(df=X_train[fin_features], quant_df=X_train[fin_features])\n",
    "email_out_flags_df = out_flag_all(df=X_train[email_features], quant_df=X_train[email_features])\n",
    "sign_flags_df = flag_signs(df=X_train[to_flag_lst])\n",
    "X_train = pd.concat(objs=[X_train, fin_quant_flags_df, email_quant_flags_df, fin_out_flags_df,\n",
    "                          email_out_flags_df, sign_flags_df], axis=1)\n",
    "\n",
    "### Flag test set.\n",
    "fin_quant_flags_df = quant_flag_all(df=X_test[fin_features], quant_df=X_train[fin_features])\n",
    "email_quant_flags_df = quant_flag_all(df=X_test[email_features], quant_df=X_train[email_features])\n",
    "fin_out_flags_df = out_flag_all(df=X_test[fin_features], quant_df=X_train[fin_features])\n",
    "email_out_flags_df = out_flag_all(df=X_test[email_features], quant_df=X_train[email_features])\n",
    "sign_flags_df = flag_signs(df=X_test[to_flag_lst])\n",
    "X_test = pd.concat(objs=[X_test, fin_quant_flags_df, email_quant_flags_df, fin_out_flags_df,\n",
    "                          email_out_flags_df, sign_flags_df], axis=1)\n",
    "\n",
    "### Flag whole set.\n",
    "fin_quant_flags_df = quant_flag_all(df=data_df[fin_features], quant_df=X_train[fin_features])\n",
    "email_quant_flags_df = quant_flag_all(df=data_df[email_features], quant_df=X_train[email_features])\n",
    "fin_out_flags_df = out_flag_all(df=data_df[fin_features], quant_df=X_train[fin_features])\n",
    "email_out_flags_df = out_flag_all(df=data_df[email_features], quant_df=X_train[email_features])\n",
    "sign_flags_df = flag_signs(df=data_df[to_flag_lst])\n",
    "data_df = pd.concat(objs=[data_df, fin_quant_flags_df, email_quant_flags_df, fin_out_flags_df,\n",
    "                          email_out_flags_df, sign_flags_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create and update feature lists.\n",
    "fin_quant_flags_lst = list(fin_quant_flags_df.columns)\n",
    "email_quant_flags_lst = list(email_quant_flags_df.columns)\n",
    "quant_flags_lst = fin_quant_flags_lst + email_quant_flags_lst\n",
    "\n",
    "fin_out_flags_lst = list(fin_out_flags_df.columns)\n",
    "email_out_flags_lst = list(email_out_flags_df.columns)\n",
    "out_flags_lst = fin_out_flags_lst + email_out_flags_lst\n",
    "\n",
    "fin_features += fin_quant_flags_lst + fin_out_flags_lst\n",
    "email_features += email_quant_flags_lst + email_out_flags_lst\n",
    "\n",
    "sign_flags_lst = list(sign_flags_df.columns)\n",
    "\n",
    "features_list = features_list + quant_flags_lst + out_flags_lst + sign_flags_lst\n",
    "\n",
    "del to_flag_lst\n",
    "del fin_quant_flags_df\n",
    "del email_quant_flags_df\n",
    "del fin_out_flags_df\n",
    "del email_out_flags_df\n",
    "del sign_flags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scale features\n",
    "### Just do min-max on floats, not bools (some are objects for now because np.nan)\n",
    "\n",
    "float_feats_lst = fin_features + email_features\n",
    "bool_feats_lst =  sign_flags_lst\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_floats = pd.DataFrame(data=scaler.fit_transform(X=X_train[float_feats_lst]),\n",
    "                            columns=float_feats_lst, index=X_train.index)\n",
    "X_train_scaled = pd.concat(objs=[train_floats, X_train[bool_feats_lst]], axis=1)\n",
    "\n",
    "test_floats = pd.DataFrame(data=scaler.transform(X=X_test[float_feats_lst]),\n",
    "                           columns=float_feats_lst,index=X_test.index)\n",
    "X_test_scaled = pd.concat(objs=[test_floats, X_test[bool_feats_lst]], axis=1)\n",
    "\n",
    "all_floats = pd.DataFrame(data=scaler.transform(X=data_df[float_feats_lst]),\n",
    "                          columns=float_feats_lst, index=data_df.index)\n",
    "data_df_scaled = pd.concat(objs=[data_df['poi'], all_floats, data_df[bool_feats_lst]], axis=1)\n",
    "\n",
    "del float_feats_lst\n",
    "del scaler\n",
    "del train_floats\n",
    "del test_floats\n",
    "del all_floats\n",
    "del X_train\n",
    "del X_test\n",
    "del data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Impute missing values:\n",
    "### Financial features to 0, email features to median, and bools to mode.\n",
    "### Restore bools to bool (from object because np.nan)\n",
    "\n",
    "imp0 = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "imp_med = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imp_mod = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "\n",
    "### Financial features to 0.\n",
    "fin_train_df = pd.DataFrame(data=imp0.fit_transform(X=X_train_scaled[fin_features]),\n",
    "                        columns=fin_features, index=X_train_scaled.index)\n",
    "fin_test_df = pd.DataFrame(data=imp0.transform(X=X_test_scaled[fin_features]),\n",
    "                       columns=fin_features, index=X_test_scaled.index)\n",
    "fin_all_df = pd.DataFrame(data=imp0.transform(X=data_df_scaled[fin_features]),\n",
    "                      columns=fin_features, index=data_df_scaled.index)\n",
    "\n",
    "### email features to median\n",
    "email_train_df = pd.DataFrame(data=imp_med.fit_transform(X=X_train_scaled[email_features]),\n",
    "                        columns=email_features, index=X_train_scaled.index)\n",
    "email_test_df = pd.DataFrame(data=imp_med.transform(X=X_test_scaled[email_features]),\n",
    "                       columns=email_features, index=X_test_scaled.index)\n",
    "email_all_df = pd.DataFrame(data=imp_med.transform(X=data_df_scaled[email_features]),\n",
    "                      columns=email_features, index=data_df_scaled.index)\n",
    "\n",
    "### Bools to mode.\n",
    "### Restore bools to bool (from object because np.nan)\n",
    "bool_train_df = (pd.DataFrame(data=imp_mod.fit_transform(X=X_train_scaled[bool_feats_lst]),\n",
    "                              columns=bool_feats_lst, index=X_train_scaled.index)).astype(bool)\n",
    "bool_test_df = pd.DataFrame(data=imp_mod.transform(X=X_test_scaled[bool_feats_lst]),\n",
    "                            columns=bool_feats_lst, index=X_test_scaled.index).astype(bool)\n",
    "bool_all_df = pd.DataFrame(data=imp_mod.transform(X=data_df_scaled[bool_feats_lst]),\n",
    "                           columns=bool_feats_lst, index=data_df_scaled.index).astype(bool)\n",
    "\n",
    "### Concat\n",
    "X_train_scaled_imp = pd.concat(objs=[fin_train_df, email_train_df, bool_train_df], axis=1)\n",
    "X_test_scaled_imp = pd.concat(objs=[fin_test_df, email_test_df, bool_test_df], axis=1)\n",
    "data_df_scaled_imp = pd.concat(objs=[data_df_scaled['poi'], fin_all_df, email_all_df, bool_all_df], axis=1)\n",
    "\n",
    "del fin_train_df\n",
    "del email_train_df\n",
    "del bool_train_df\n",
    "del fin_test_df\n",
    "del email_test_df\n",
    "del bool_test_df\n",
    "del fin_all_df\n",
    "del email_all_df\n",
    "del bool_all_df\n",
    "del bool_feats_lst\n",
    "del X_train_scaled\n",
    "del X_test_scaled\n",
    "del data_df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sklearn predictions as features\n",
    "\n",
    "# 1) Kmeans cluster.\n",
    "train_cluster_subspace, test_cluster_subspace \\\n",
    "    = add_k_means_n(X_train=X_train_scaled_imp, X_test=X_test_scaled_imp)\n",
    "X_train_scaled_imp_k = pd.concat(objs=[X_train_scaled_imp, train_cluster_subspace], axis=1)\n",
    "X_test_scaled_imp_k = pd.concat(objs=[X_test_scaled_imp, test_cluster_subspace], axis=1)\n",
    "\n",
    "train_cluster_subspace, test_cluster_subspace \\\n",
    "    = add_k_means_n(X_train=X_train_scaled_imp, X_test=data_df_scaled_imp[features_list[1:]])\n",
    "data_df_scaled_imp_k = pd.concat(objs=[data_df_scaled_imp, test_cluster_subspace], axis=1)\n",
    "\n",
    "k_means_feats_lst = k_means_feats_lst = list(train_cluster_subspace.columns)\n",
    "features_list += k_means_feats_lst\n",
    "\n",
    "del train_cluster_subspace\n",
    "del test_cluster_subspace\n",
    "del X_train_scaled_imp\n",
    "del X_test_scaled_imp\n",
    "del data_df_scaled_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " base_perf_engineered\n",
      "\n",
      " dt_clf\n",
      "DecisionTreeClassifier()\n",
      "Training time: 0.007 s\n",
      "Prediction time: 0.003 s\n",
      "Confusion matrix:\n",
      " [[19  3]\n",
      " [ 2  2]]\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.9047619, 0.4      ]), array([0.86363636, 0.5       ]), array([0.88372093, 0.44444444]), array([22,  4], dtype=int64))\n",
      "\n",
      " rf_clf\n",
      "RandomForestClassifier()\n",
      "Training time: 0.117 s\n",
      "Prediction time: 0.009 s\n",
      "Confusion matrix:\n",
      " [[22  0]\n",
      " [ 4  0]]\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.84615385, 0.        ]), array([1., 0.]), array([0.91666667, 0.        ]), array([22,  4], dtype=int64))\n",
      "\n",
      " ab_clf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaleb\\anaconda3\\envs\\py3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier()\n",
      "Training time: 0.118 s\n",
      "Prediction time: 0.017 s\n",
      "Confusion matrix:\n",
      " [[20  2]\n",
      " [ 3  1]]\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.86956522, 0.33333333]), array([0.90909091, 0.25      ]), array([0.88888889, 0.28571429]), array([22,  4], dtype=int64))\n",
      "\n",
      " kn_clf\n",
      "KNeighborsClassifier()\n",
      "Training time: 0.004 s\n",
      "Prediction time: 0.005 s\n",
      "Confusion matrix:\n",
      " [[21  1]\n",
      " [ 3  1]]\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.875, 0.5  ]), array([0.95454545, 0.25      ]), array([0.91304348, 0.33333333]), array([22,  4], dtype=int64))\n",
      "\n",
      " gnb_clf\n",
      "GaussianNB()\n",
      "Training time: 0.004 s\n",
      "Prediction time: 0.003 s\n",
      "Confusion matrix:\n",
      " [[18  4]\n",
      " [ 2  2]]\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.9       , 0.33333333]), array([0.81818182, 0.5       ]), array([0.85714286, 0.4       ]), array([22,  4], dtype=int64))\n",
      "\n",
      " svc_clf\n",
      "SVC()\n",
      "Training time: 0.006 s\n",
      "Prediction time: 0.003 s\n",
      "Confusion matrix:\n",
      " [[22  0]\n",
      " [ 4  0]]\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.84615385, 0.        ]), array([1., 0.]), array([0.91666667, 0.        ]), array([22,  4], dtype=int64))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaleb\\anaconda3\\envs\\py3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "### Construct baseline performance with all features before tuning/selection.\n",
    "### Split train set again for a baseline model that won't touch the final test set.\n",
    "X_train_base, X_test_base, y_train_base, y_test_base \\\n",
    "    = train_test_split(X_train_scaled_imp_k, y_train, test_size=.3, random_state=42)\n",
    "y_train_1d_base = np.ravel(y_train_base.astype(bool))\n",
    "y_test_1d_base = np.ravel(y_test_base.astype(bool))\n",
    "\n",
    "base_perf_engineered_df = pd.DataFrame(columns=ordered_cols_lst)\n",
    "\n",
    "base_perfs_dict = {'base_perf_engineered': base_perf_engineered_df}\n",
    "imp_sets_dict = {'base_perf_engineered': [X_train_base, X_test_base]}\n",
    "\n",
    "### Modifies the base_perfs_dict in place, since dict has no deep copy method.\n",
    "get_base_perfs(base_perfs_dict=base_perfs_dict, imp_sets_dict=imp_sets_dict, clf_dict=clf_dict, y_train=y_train_1d_base,\n",
    "               y_test=y_test_1d_base)\n",
    "\n",
    "base_perfs_dict['first_base'] = base_perf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0 sel_per_empty_rf_clf \n",
      "\n",
      "Fitting 5 folds for each of 57600 candidates, totalling 288000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 160 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done 928 tasks      | elapsed:   16.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1504 tasks      | elapsed:   25.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2208 tasks      | elapsed:   36.4s\n",
      "[Parallel(n_jobs=-1)]: Done 3040 tasks      | elapsed:   49.9s\n",
      "[Parallel(n_jobs=-1)]: Done 4000 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 5088 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 6304 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 7648 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 9120 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 10720 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 12448 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 14304 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 16288 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 18400 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 20640 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 23008 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 25504 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done 28128 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 30880 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done 33760 tasks      | elapsed:  8.4min\n",
      "[Parallel(n_jobs=-1)]: Done 36768 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done 39904 tasks      | elapsed: 10.0min\n",
      "[Parallel(n_jobs=-1)]: Done 43168 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done 46560 tasks      | elapsed: 11.8min\n",
      "[Parallel(n_jobs=-1)]: Done 50080 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=-1)]: Done 53728 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=-1)]: Done 57504 tasks      | elapsed: 14.5min\n",
      "[Parallel(n_jobs=-1)]: Done 61408 tasks      | elapsed: 15.5min\n",
      "[Parallel(n_jobs=-1)]: Done 65440 tasks      | elapsed: 16.4min\n",
      "[Parallel(n_jobs=-1)]: Done 69600 tasks      | elapsed: 17.5min\n",
      "[Parallel(n_jobs=-1)]: Done 73888 tasks      | elapsed: 18.5min\n",
      "[Parallel(n_jobs=-1)]: Done 78304 tasks      | elapsed: 19.6min\n",
      "[Parallel(n_jobs=-1)]: Done 82848 tasks      | elapsed: 20.8min\n",
      "[Parallel(n_jobs=-1)]: Done 87520 tasks      | elapsed: 21.9min\n",
      "[Parallel(n_jobs=-1)]: Done 92320 tasks      | elapsed: 23.1min\n",
      "[Parallel(n_jobs=-1)]: Done 97248 tasks      | elapsed: 24.3min\n",
      "[Parallel(n_jobs=-1)]: Done 102304 tasks      | elapsed: 25.6min\n",
      "[Parallel(n_jobs=-1)]: Done 107488 tasks      | elapsed: 26.9min\n",
      "[Parallel(n_jobs=-1)]: Done 112800 tasks      | elapsed: 28.2min\n",
      "[Parallel(n_jobs=-1)]: Done 118240 tasks      | elapsed: 29.6min\n",
      "[Parallel(n_jobs=-1)]: Done 123808 tasks      | elapsed: 31.0min\n",
      "[Parallel(n_jobs=-1)]: Done 129504 tasks      | elapsed: 32.3min\n",
      "[Parallel(n_jobs=-1)]: Done 135328 tasks      | elapsed: 33.8min\n",
      "[Parallel(n_jobs=-1)]: Done 141280 tasks      | elapsed: 35.3min\n",
      "[Parallel(n_jobs=-1)]: Done 147360 tasks      | elapsed: 36.7min\n",
      "[Parallel(n_jobs=-1)]: Done 153568 tasks      | elapsed: 38.2min\n",
      "[Parallel(n_jobs=-1)]: Done 159904 tasks      | elapsed: 39.7min\n",
      "[Parallel(n_jobs=-1)]: Done 166368 tasks      | elapsed: 41.3min\n",
      "[Parallel(n_jobs=-1)]: Done 172960 tasks      | elapsed: 42.8min\n",
      "[Parallel(n_jobs=-1)]: Done 179680 tasks      | elapsed: 44.4min\n",
      "[Parallel(n_jobs=-1)]: Done 186528 tasks      | elapsed: 46.0min\n",
      "[Parallel(n_jobs=-1)]: Done 193504 tasks      | elapsed: 47.6min\n",
      "[Parallel(n_jobs=-1)]: Done 200608 tasks      | elapsed: 49.3min\n",
      "[Parallel(n_jobs=-1)]: Done 207840 tasks      | elapsed: 51.0min\n",
      "[Parallel(n_jobs=-1)]: Done 215200 tasks      | elapsed: 52.8min\n",
      "[Parallel(n_jobs=-1)]: Done 222688 tasks      | elapsed: 54.5min\n",
      "[Parallel(n_jobs=-1)]: Done 230304 tasks      | elapsed: 56.3min\n",
      "[Parallel(n_jobs=-1)]: Done 238048 tasks      | elapsed: 58.1min\n",
      "[Parallel(n_jobs=-1)]: Done 245920 tasks      | elapsed: 60.0min\n",
      "[Parallel(n_jobs=-1)]: Done 253920 tasks      | elapsed: 61.9min\n",
      "[Parallel(n_jobs=-1)]: Done 262048 tasks      | elapsed: 63.8min\n",
      "[Parallel(n_jobs=-1)]: Done 270304 tasks      | elapsed: 65.8min\n",
      "[Parallel(n_jobs=-1)]: Done 278688 tasks      | elapsed: 67.7min\n",
      "[Parallel(n_jobs=-1)]: Done 287200 tasks      | elapsed: 69.7min\n",
      "[Parallel(n_jobs=-1)]: Done 288000 out of 288000 | elapsed: 69.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........... (step 1 of 2) Processing sel_per, total=   0.1s\n",
      "[Pipeline] ............ (step 2 of 2) Processing rf_clf, total=   0.0s\n",
      "\n",
      " GridSearchCV(estimator=Pipeline(steps=[('sel_per', SelectPercentile()),\n",
      "                                       ('rf_clf', RandomForestClassifier())],\n",
      "                                verbose=True),\n",
      "             n_jobs=-1,\n",
      "             param_grid={'rf_clf__bootstrap': [True, False],\n",
      "                         'rf_clf__max_depth': [8, 16, 24, 32],\n",
      "                         'rf_clf__max_features': ['sqrt', 'log2'],\n",
      "                         'rf_clf__min_samples_leaf': [2, 4, 8, 16],\n",
      "                         'rf_clf__min_samples_split': [2, 4, 8],\n",
      "                         'rf_clf__n_estimators': [6, 8,...\n",
      "                         'rf_clf__n_jobs': [-1], 'rf_clf__random_state': [42],\n",
      "                         'sel_per__percentile': array([  5,  10,  15,  20,  25,  30,  35,  40,  45,  50,  55,  60,  65,\n",
      "        70,  75,  80,  85,  90,  95, 100]),\n",
      "                         'sel_per__score_func': [<function f_classif at 0x00000238BDB17430>,\n",
      "                                                 <function chi2 at 0x00000238BDB17790>,\n",
      "                                                 functools.partial(<function mutual_info_classif at 0x00000238BDB4D310>, random_state=42)]},\n",
      "             scoring='recall_weighted', verbose=3)\n",
      "\n",
      "best_score_: 0.8843137254901962\n",
      "\n",
      "best_params_: {'rf_clf__bootstrap': False, 'rf_clf__max_depth': 8, 'rf_clf__max_features': 'sqrt', 'rf_clf__min_samples_leaf': 2, 'rf_clf__min_samples_split': 8, 'rf_clf__n_estimators': 6, 'rf_clf__n_jobs': -1, 'rf_clf__random_state': 42, 'sel_per__percentile': 90, 'sel_per__score_func': functools.partial(<function mutual_info_classif at 0x00000238BDB4D310>, random_state=42)}\n"
     ]
    }
   ],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "### Because the proliferation of features has led to overfit\n",
    "### (see gridsearch notebooks in the supplemental material folder),\n",
    "### I will remove the quantile flags, the outlier flags, the sign flags, and the cluster flags,\n",
    "### leaving the original base features (not already removed) and the ratio features.\n",
    "\n",
    "drop_lst = quant_flags_lst + out_flags_lst + sign_flags_lst + k_means_feats_lst\n",
    "keep_lst = [feat for feat in features_list[1:] if feat not in drop_lst]\n",
    "\n",
    "X_train_trimmed = X_train_scaled_imp_k[keep_lst]\n",
    "\n",
    "\n",
    "n_jobs = -1\n",
    "\n",
    "### Callables to pass into algorithms.\n",
    "mutual_info_classif_partial = partial(mutual_info_classif, random_state=42)\n",
    "DecisionTreeClassifier_partial = partial(DecisionTreeClassifier, random_state=42)\n",
    "RandomForestClassifier_partial = partial(RandomForestClassifier, random_state=42, n_jobs=n_jobs)\n",
    "AdaBoostClassifier_partial = partial(AdaBoostClassifier, random_state=42)\n",
    "svm_SVC_partial = partial(svm.SVC, random_state=42)\n",
    "KNeighborsClassifier_partial = partial(KNeighborsClassifier, n_jobs=n_jobs)\n",
    "\n",
    "selectors = {\n",
    "    'sel_per': {\n",
    "        'sel': SelectPercentile(),\n",
    "        'params': {\n",
    "            'sel_per__score_func': [f_classif, chi2, mutual_info_classif_partial],\n",
    "            'sel_per__percentile': np.arange(start=5, stop=105, step=5)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "decomps = {\n",
    "    'empty' : None\n",
    "#     'fica': {\n",
    "#         'dec': FastICA(),\n",
    "#         'params': {\n",
    "#             'fica__algorithm': ['parallel', 'deflation'],\n",
    "#             'fica__fun': ['logcosh', 'exp', 'cube'],\n",
    "#             'fica__random_state': [42]\n",
    "#         }\n",
    "#     },\n",
    "#         'ipca': {\n",
    "#         'dec': IncrementalPCA(),\n",
    "#         'params': {\n",
    "#             ### defaults\n",
    "#         }\n",
    "#     },\n",
    "#     'kpca': {\n",
    "#         'dec': KernelPCA(),\n",
    "#         'params': {\n",
    "#             'kpca__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'cosine',\n",
    "#                              'precomputed'],\n",
    "#             'kpca__random_state': [42],\n",
    "#             'kpca__n_jobs': [n_jobs]\n",
    "#         }\n",
    "#     },\n",
    "    ### PCA kept throwing an error that the data contained nans, inf,\n",
    "    ### or too large dtypes, despite no nans, infs, or wrong types per\n",
    "    ### replications of the sklearn (and numpy) condition checks that threw the\n",
    "    ### errors (\"errors\" because using PCA threw the error from sklearn script,\n",
    "    ### but further testing showed that PCA's get_precision method may have\n",
    "    ### thrown it from a NumPy script (see cells below for tracing), and that\n",
    "    ### did not include dtype size).\n",
    "    ### Maybe (probably?) a problem with the transformed data handed off from\n",
    "    ### SelectPercentile, but I'm done testing. Just need to finish. Skip PCA.\n",
    "#     'pca': {\n",
    "#         'dec': PCA(),\n",
    "#         'params': {\n",
    "#             'pca__random_state': [42]\n",
    "#         }\n",
    "#     },\n",
    "#     'tsvd': {\n",
    "#         'dec': TruncatedSVD(),\n",
    "#         'params': {\n",
    "#             'tsvd__n_components': [2, 4, 8, 16, 32, 64, 128],\n",
    "#             'tsvd__algorithm': ['arpack', 'randomized'],\n",
    "#             'tsvd__random_state': [42]\n",
    "#         }\n",
    "#     }\n",
    "}\n",
    "\n",
    "classifiers = {\n",
    "#     'dt_clf': {\n",
    "#         'clf': DecisionTreeClassifier(),\n",
    "#         'params': {\n",
    "#             'dt_clf__random_state': [42]\n",
    "#         }\n",
    "#     },\n",
    "    'rf_clf': {\n",
    "        'clf': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'rf_clf__n_estimators': [6, 8, 10, 12, 14],\n",
    "            'rf_clf__max_features': ['sqrt', 'log2'],\n",
    "            'rf_clf__max_depth': [8, 16, 24, 32],\n",
    "            'rf_clf__min_samples_split': [2, 4, 8],\n",
    "            'rf_clf__min_samples_leaf': [2, 4, 8, 16],\n",
    "            'rf_clf__bootstrap': [True, False],\n",
    "            'rf_clf__random_state': [42],\n",
    "            'rf_clf__n_jobs': [n_jobs]\n",
    "        }\n",
    "    },\n",
    "#     'ab_clf': {\n",
    "#         'clf': AdaBoostClassifier(),\n",
    "#         'params': {\n",
    "#             'ab_clf__base_estimator': [\n",
    "#                 DecisionTreeClassifier_partial(),\n",
    "#                 RandomForestClassifier_partial(),\n",
    "#                 AdaBoostClassifier_partial(),\n",
    "#                 svm_SVC_partial(),\n",
    "#                 KNeighborsClassifier_partial(),\n",
    "#                 GaussianNB()\n",
    "#             ],\n",
    "#             'ab_clf__n_estimators': [8, 16, 24, 32, 40, 48, 56],\n",
    "#             'ab_clf__algorithm': ['SAMME', 'SAMME.R'],\n",
    "#             'ab_clf__random_state': [42]\n",
    "#         }\n",
    "#     },\n",
    "#     'kn_clf': {\n",
    "#         'clf': KNeighborsClassifier(),\n",
    "#         'params': {\n",
    "#             'kn_clf__n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "#             'kn_clf__weights': ['uniform', 'distance'],\n",
    "#             'kn_clf__algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "#             'kn_clf__leaf_size': [4, 8, 12, 16, 20, 24, 30],\n",
    "#             'kn_clf__n_jobs': [n_jobs]\n",
    "#         }\n",
    "#     },\n",
    "#     'gnb_clf': {\n",
    "#         'clf': GaussianNB(),\n",
    "#         'params': {\n",
    "#             # Defaults\n",
    "#         }\n",
    "#     }\n",
    "}\n",
    "\n",
    "\n",
    "imp_gscvs_dict = {}\n",
    "imp_gscvs_dict['mixed_impute_trimmed'] \\\n",
    "    = search_em_all(X_train=X_train_trimmed, y_train=y_train_1d, selectors=selectors,\n",
    "                    decomps=decomps, classifiers=classifiers, pipe_verbose=True,\n",
    "                    scoring='recall_weighted', n_jobs=-1)\n",
    "### Can try with multiple datasets for comparison.\n",
    "# imp_gscvs_dict['other_set'] \\\n",
    "#     = search_em_all(X_train=X_train_other_set, y_train=y_train_1d, selectors=selectors,\n",
    "#                     decomps=decomps, classifiers=classifiers, pipe_verbose=True,\n",
    "#                     scoring='recall_weighted', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/imp_gscvs_dict6.pkl', 'wb') as file:\n",
    "    pickle.dump(obj=imp_gscvs_dict, file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mixed_impute_trimmed': {'sel_per_empty_rf_clf': GridSearchCV(estimator=Pipeline(steps=[('sel_per', SelectPercentile()),\n",
       "                                         ('rf_clf', RandomForestClassifier())],\n",
       "                                  verbose=True),\n",
       "               n_jobs=-1,\n",
       "               param_grid={'rf_clf__bootstrap': [True, False],\n",
       "                           'rf_clf__max_depth': [8, 16, 24, 32],\n",
       "                           'rf_clf__max_features': ['sqrt', 'log2'],\n",
       "                           'rf_clf__min_samples_leaf': [2, 4, 8, 16],\n",
       "                           'rf_clf__min_samples_split': [2, 4, 8],\n",
       "                           'rf_clf__n_estimators': [6, 8,...\n",
       "                           'rf_clf__n_jobs': [-1], 'rf_clf__random_state': [42],\n",
       "                           'sel_per__percentile': array([  5,  10,  15,  20,  25,  30,  35,  40,  45,  50,  55,  60,  65,\n",
       "          70,  75,  80,  85,  90,  95, 100]),\n",
       "                           'sel_per__score_func': [<function f_classif at 0x00000238BDB17430>,\n",
       "                                                   <function chi2 at 0x00000238BDB17790>,\n",
       "                                                   functools.partial(<function mutual_info_classif at 0x00000238BDB4D310>, random_state=42)]},\n",
       "               scoring='recall_weighted', verbose=3)}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_gscvs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sel_per_empty_rf_clf \n",
      "\n",
      "Best score:\n",
      " 0.8843137254901962 \n",
      "\n",
      "Best estimator:\n",
      " Pipeline(steps=[('sel_per',\n",
      "                 SelectPercentile(percentile=90,\n",
      "                                  score_func=functools.partial(<function mutual_info_classif at 0x00000238BDB4D310>, random_state=42))),\n",
      "                ('rf_clf',\n",
      "                 RandomForestClassifier(bootstrap=False, max_depth=8,\n",
      "                                        max_features='sqrt', min_samples_leaf=2,\n",
      "                                        min_samples_split=8, n_estimators=6,\n",
      "                                        n_jobs=-1, random_state=42))],\n",
      "         verbose=True) \n",
      "\n",
      "[Pipeline] ........... (step 1 of 2) Processing sel_per, total=   0.8s\n",
      "[Pipeline] ............ (step 2 of 2) Processing rf_clf, total=   0.0s\n",
      "Confusion matrix:\n",
      " [[32  2]\n",
      " [ 3  0]] \n",
      "\n",
      "Precision, recall, f beta score, support:\n",
      " (array([0.91428571, 0.        ]), array([0.94117647, 0.        ]), array([0.92753623, 0.        ]), array([34,  3], dtype=int64)) \n",
      "\n",
      "Custom F beta using nonPOI precision and POI recall:\n",
      " 0.0 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_f = lambda precision, recall: 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "for name, gscv in imp_gscvs_dict['mixed_impute_trimmed'].items():\n",
    "    print(name, '\\n')\n",
    "    print('Best score:\\n', gscv.best_score_, '\\n')\n",
    "    print('Best estimator:\\n', gscv.best_estimator_, '\\n')\n",
    "    clf = gscv.best_estimator_.fit(X=X_train_scaled_imp_k, y=y_train_1d)\n",
    "    pred = clf.predict(X_test_scaled_imp_k)\n",
    "    conf = confusion_matrix(y_true=y_test_1d, y_pred=pred)\n",
    "    print('Confusion matrix:\\n', conf, '\\n')\n",
    "    prf = precision_recall_fscore_support(y_true=y_test_1d, y_pred=pred)\n",
    "    print('Precision, recall, f beta score, support:\\n', prf, '\\n')\n",
    "    print('Custom F beta using nonPOI precision and POI recall:\\n', get_f(prf[0][0], prf[1][1]), '\\n')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "features_list = keep_lst\n",
    "my_dataset = df.T.to_dict()\n",
    "clf = imp_gscvs_dict['mixed_impute']['<name>_clf']\n",
    "\n",
    "CLF_PICKLE_FILENAME = \"data/my_classifier.pkl\"\n",
    "DATASET_PICKLE_FILENAME = \"data/my_dataset.pkl\"\n",
    "FEATURE_LIST_FILENAME = \"data/my_feature_list.pkl\"\n",
    "\n",
    "with open(CLF_PICKLE_FILENAME, \"w\") as clf_outfile:\n",
    "    pickle.dump(clf, clf_outfile)\n",
    "with open(DATASET_PICKLE_FILENAME, \"w\") as dataset_outfile:\n",
    "    pickle.dump(my_dataset, dataset_outfile)\n",
    "with open(FEATURE_LIST_FILENAME, \"w\") as featurelist_outfile:\n",
    "    pickle.dump(features_list, featurelist_outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tester import dump_classifier_and_data\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "import tester"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3] *",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
